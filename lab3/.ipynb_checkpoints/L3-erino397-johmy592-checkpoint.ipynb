{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**Due date:** 2018-02-09\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L3: Part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging is the task of labelling words (tokens) with parts of speech such as noun, adjective, and verb. In this lab you will implement a POS tagger based on the averaged perceptron and evaluate it on the English treebank from the [Universal Dependencies Project](http://universaldependencies.org), a corpus containing more than 16,000 sentences (254,000&nbsp;tokens) annotated with, among others, parts of speech. The data is divided into two files:\n",
    "\n",
    "<table align=\"left\">\n",
    "<tr><td><code>train.txt</code></td><td style=\"text-align: right\">12,543 sentences</td><td style=\"text-align: right\">204,585 tokens</td></tr>\n",
    "<tr><td><code>dev.txt</code></td><td style=\"text-align: right\">2,002 sentences</td><td style=\"text-align: right\">25,148 tokens</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the Python module that is required for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp3\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell loads the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = nlp3.read_data(\"/home/TDDE09/labs/l3/data/train.txt\")\n",
    "dev_data = nlp3.read_data(\"/home/TDDE09/labs/l3/data/dev.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both data sets consist of tagged sentences. In Python, a tagged sentence is represented as a list of string pairs, where the first component of each pair represents a word and the second component represents a part-of-speech tag. Run the following code cell to see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'PRON'),\n",
       " ('has', 'AUX'),\n",
       " ('been', 'VERB'),\n",
       " ('talk', 'NOUN'),\n",
       " ('that', 'SCONJ'),\n",
       " ('the', 'DET'),\n",
       " ('night', 'NOUN'),\n",
       " ('curfew', 'NOUN'),\n",
       " ('might', 'AUX'),\n",
       " ('be', 'AUX'),\n",
       " ('implemented', 'VERB'),\n",
       " ('again', 'ADV'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags are explained and exemplified in the [Annotation Guidelines](http://universaldependencies.org/u/pos/all.html) of the Universal Dependencies Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next code cell to train the default tagger, tag the sample sentence from above, and evaluate the tagger on the development data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 99.99%\n",
      "Accuracy: 87.63%\n"
     ]
    }
   ],
   "source": [
    "tagger = nlp3.Tagger()\n",
    "tagger.train(training_data)\n",
    "print(\"Accuracy: {:.2%}\".format(nlp3.accuracy(tagger, dev_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your main task in this lab is to re-implement the default tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 1</div>\n",
    "<div class=\"panel-body\">\n",
    "Implement a part-of-speech tagger based on the averaged perceptron, train it on the training data, and evaluate performance on the development data. Your tagger should get the same results as the default tagger.\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "Starter code for this problem is given in the following code cell. The provided class simply inherits from `nlp3.Tagger` and calls the methods in the superclass. Your task is to replace these calls with your own code. The intended interface of the methods is documented in the docstrings.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "You will not need to touch the method `features()`, unless you want to do the advanced part of this lab (see below).\n",
    "</div>\n",
    "\n",
    "You are allowed to use the provided `nlp3.Perceptron` class for the implementation of the multi-class perceptron. This class has the same interface as the class that you implemented in lab&nbsp;L1, except for one additional method `finalize()`. This method implements the last step of the training method, the averaging of the classifier&rsquo;s weight vector. If you feel adventurous, then you may want to try using your own implementation instead of the provided one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalize\n",
      "predict\n",
      "update\n"
     ]
    }
   ],
   "source": [
    "for m in dir(nlp3.Perceptron):\n",
    "    if not m.startswith('_'):\n",
    "        print (m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tagger():\n",
    "    \"\"\"A part-of-speech tagger based on a multi-class perceptron\n",
    "    classifier.\n",
    "\n",
    "    This tagger implements a simple, left-to-right tagging algorithm\n",
    "    where the prediction of the tag for the next word in the sentence\n",
    "    can be based on the surrounding words and the previously\n",
    "    predicted tags. The exact features that this prediction is based\n",
    "    on can be controlled with the `features()` method, which should\n",
    "    return a feature vector that can be used as an input to the\n",
    "    multi-class perceptron.\n",
    "\n",
    "    Attributes:\n",
    "        classifier: A multi-class perceptron classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises a new tagger.\"\"\"\n",
    "        self.classifier = nlp3.Perceptron()\n",
    "\n",
    "    def features(self, words, i, pred_tags):\n",
    "        \"\"\"Extracts features for the specified tagger configuration.\n",
    "        \n",
    "        Args:\n",
    "            words: The input sentence, a list of words.\n",
    "            i: The index of the word that is currently being tagged.\n",
    "            pred_tags: The list of previously predicted tags.\n",
    "        \n",
    "        Returns:\n",
    "            A feature vector for the specified configuration.\n",
    "        \"\"\"\n",
    "        features = ['word:%s' % words[i].lower()]\n",
    "        \n",
    "        # Pair of two previous tags\n",
    "        prev_tag = 'prevtags:%s' % pred_tags[-2:] if len(pred_tags) >= 2 else 'prevtags:None'\n",
    "        features.append(prev_tag)\n",
    "        \n",
    "        # Previous word (lowercased)\n",
    "        prev_word = 'prevword:%s' % words[i-1].lower() if i > 0 else 'prevword:None'\n",
    "        features.append(prev_word)\n",
    "        \n",
    "        # Next word (lowercased)\n",
    "        next_word = 'nextword:%s' % words[i+1].lower() if i < len(words) - 1 else 'nextword:None'\n",
    "        features.append(next_word)\n",
    "        \n",
    "        return features\n",
    "\n",
    "\n",
    "    def tag(self, words):\n",
    "        \"\"\"Tags a sentence with part-of-speech tags.\n",
    "\n",
    "        Args:\n",
    "            words: The input sentence, a list of words.\n",
    "\n",
    "        Returns:\n",
    "            The list of predicted tags for the input sentence.\n",
    "        \"\"\"\n",
    "        \n",
    "        pred_tags = []\n",
    "        for i in range(len(words)):\n",
    "            features = self.features(words, i, pred_tags)\n",
    "            pred_tags.append(self.classifier.predict(features))\n",
    "        \n",
    "        return pred_tags\n",
    "\n",
    "    def update(self, words, gold_tags):\n",
    "        \"\"\"Updates the tagger with a single training instance.\n",
    "\n",
    "        Args:\n",
    "            words: The list of words in the input sentence.\n",
    "            gold_tags: The list of gold-standard tags for the input\n",
    "                sentence.\n",
    "\n",
    "        Returns:\n",
    "            The list of predicted tags for the input sentence.\n",
    "        \"\"\"\n",
    "        pred_tags = []           \n",
    "        for i in range(len(words)):\n",
    "            features = self.features(words, i, pred_tags)\n",
    "            pred_tags.append(self.classifier.update(features, gold_tags[i]))\n",
    "                     \n",
    "        return pred_tags\n",
    "\n",
    "    def train(self, data):\n",
    "        \"\"\"Train a new tagger on training data.\n",
    "\n",
    "        Args:\n",
    "            data: Training data, a list of tagged sentences.\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract word and tag samples\n",
    "        X, y = [], []\n",
    "        for sample in data:\n",
    "            X.append([word for word, _ in sample])\n",
    "            y.append([pos_tag for _, pos_tag in sample])\n",
    "        \n",
    "        # To try with different amount of epochs\n",
    "        for _ in range(1):\n",
    "            # For each sample make update\n",
    "            for i, (words, gold_tags) in enumerate(zip(X, y)):\n",
    "                self.update(words, gold_tags)\n",
    "            \n",
    "        # Average weights\n",
    "        self.finalize()\n",
    "\n",
    "    def finalize(self):\n",
    "        \"\"\"Finalizes the classifier by averaging its weight vectors.\"\"\"\n",
    "        self.classifier.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to test your tagger. At the end of the lab you should get the same results as in the evaluation of the default tagger (assuming that you do not change the feature extraction, see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.39%\n"
     ]
    }
   ],
   "source": [
    "our_tagger = Tagger()\n",
    "our_tagger.train(training_data)\n",
    "print(\"Accuracy: {:.2%}\".format(nlp3.accuracy(our_tagger, dev_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows, we try to give you an idea of what the two methods `train()` and `tag()` do. We start with the latter.\n",
    "\n",
    "### Tagging\n",
    "\n",
    "The default tagger implements the sequence model presented in the lecture. In this model, sentences are tagged from left to right. A **configuration** of the tagger consists of the list of words, the index of the current word, and the list of already predicted tags. For each word in the sentence, the tagger calls the method `features()` to obtain a feature vector for the current configuration. To illustrate how this works, we define a variant of the default tagger that only extracts a single feature, the current word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoTagger(nlp3.Tagger):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.debug = False\n",
    "    \n",
    "    def features(self, words, i, pred_tags):\n",
    "        features = [words[i]]\n",
    "        if self.debug:\n",
    "            print(\"words: {}\".format(\" \".join(words)))\n",
    "            print(\"i: {} (current word: {})\".format(i, words[i]))\n",
    "            print(\"pred_tags: {}\".format(\" \".join(pred_tags)))\n",
    "            print(\"features: {}\".format(\" \".join(features)))\n",
    "            print()\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train this tagger and evaluate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Progress: 0.00%\r",
      "Progress: 0.01%\r",
      "Progress: 0.02%\r",
      "Progress: 0.02%\r",
      "Progress: 0.03%\r",
      "Progress: 0.04%\r",
      "Progress: 0.05%\r",
      "Progress: 0.06%\r",
      "Progress: 0.06%\r",
      "Progress: 0.07%\r",
      "Progress: 0.08%\r",
      "Progress: 0.09%\r",
      "Progress: 0.10%\r",
      "Progress: 0.10%\r",
      "Progress: 0.11%\r",
      "Progress: 0.12%\r",
      "Progress: 0.13%\r",
      "Progress: 0.14%\r",
      "Progress: 0.14%\r",
      "Progress: 0.15%\r",
      "Progress: 0.16%\r",
      "Progress: 0.17%\r",
      "Progress: 0.18%\r",
      "Progress: 0.18%\r",
      "Progress: 0.19%\r",
      "Progress: 0.20%\r",
      "Progress: 0.21%\r",
      "Progress: 0.22%\r",
      "Progress: 0.22%\r",
      "Progress: 0.23%\r",
      "Progress: 0.24%\r",
      "Progress: 0.25%\r",
      "Progress: 0.26%\r",
      "Progress: 0.26%\r",
      "Progress: 0.27%\r",
      "Progress: 0.28%\r",
      "Progress: 0.29%\r",
      "Progress: 0.29%\r",
      "Progress: 0.30%\r",
      "Progress: 0.31%\r",
      "Progress: 0.32%\r",
      "Progress: 0.33%\r",
      "Progress: 0.33%\r",
      "Progress: 0.34%\r",
      "Progress: 0.35%\r",
      "Progress: 0.36%\r",
      "Progress: 0.37%\r",
      "Progress: 0.37%\r",
      "Progress: 0.38%\r",
      "Progress: 0.39%\r",
      "Progress: 0.40%\r",
      "Progress: 0.41%\r",
      "Progress: 0.41%\r",
      "Progress: 0.42%\r",
      "Progress: 0.43%\r",
      "Progress: 0.44%\r",
      "Progress: 0.45%\r",
      "Progress: 0.45%\r",
      "Progress: 0.46%\r",
      "Progress: 0.47%\r",
      "Progress: 0.48%\r",
      "Progress: 0.49%\r",
      "Progress: 0.49%\r",
      "Progress: 0.50%\r",
      "Progress: 0.51%\r",
      "Progress: 0.52%\r",
      "Progress: 0.53%\r",
      "Progress: 0.53%\r",
      "Progress: 0.54%\r",
      "Progress: 0.55%\r",
      "Progress: 0.56%\r",
      "Progress: 0.57%\r",
      "Progress: 0.57%\r",
      "Progress: 0.58%\r",
      "Progress: 0.59%\r",
      "Progress: 0.60%\r",
      "Progress: 0.61%\r",
      "Progress: 0.61%\r",
      "Progress: 0.62%\r",
      "Progress: 0.63%\r",
      "Progress: 0.64%\r",
      "Progress: 0.65%\r",
      "Progress: 0.65%\r",
      "Progress: 0.66%\r",
      "Progress: 0.67%\r",
      "Progress: 0.68%\r",
      "Progress: 0.69%\r",
      "Progress: 0.69%\r",
      "Progress: 0.70%\r",
      "Progress: 0.71%\r",
      "Progress: 0.72%\r",
      "Progress: 0.73%\r",
      "Progress: 0.73%\r",
      "Progress: 0.74%\r",
      "Progress: 0.75%\r",
      "Progress: 0.76%\r",
      "Progress: 0.77%\r",
      "Progress: 0.77%\r",
      "Progress: 0.78%\r",
      "Progress: 0.79%\r",
      "Progress: 0.80%\r",
      "Progress: 0.81%\r",
      "Progress: 0.81%\r",
      "Progress: 0.82%\r",
      "Progress: 0.83%\r",
      "Progress: 0.84%\r",
      "Progress: 0.85%\r",
      "Progress: 0.85%\r",
      "Progress: 0.86%\r",
      "Progress: 0.87%\r",
      "Progress: 0.88%\r",
      "Progress: 0.88%\r",
      "Progress: 0.89%\r",
      "Progress: 0.90%\r",
      "Progress: 0.91%\r",
      "Progress: 0.92%\r",
      "Progress: 0.92%\r",
      "Progress: 0.93%\r",
      "Progress: 0.94%\r",
      "Progress: 0.95%\r",
      "Progress: 0.96%\r",
      "Progress: 0.96%\r",
      "Progress: 0.97%\r",
      "Progress: 0.98%\r",
      "Progress: 0.99%\r",
      "Progress: 1.00%\r",
      "Progress: 1.00%\r",
      "Progress: 1.01%\r",
      "Progress: 1.02%\r",
      "Progress: 1.03%\r",
      "Progress: 1.04%\r",
      "Progress: 1.04%\r",
      "Progress: 1.05%\r",
      "Progress: 1.06%\r",
      "Progress: 1.07%\r",
      "Progress: 1.08%\r",
      "Progress: 1.08%\r",
      "Progress: 1.09%\r",
      "Progress: 1.10%\r",
      "Progress: 1.11%\r",
      "Progress: 1.12%\r",
      "Progress: 1.12%\r",
      "Progress: 1.13%\r",
      "Progress: 1.14%\r",
      "Progress: 1.15%\r",
      "Progress: 1.16%\r",
      "Progress: 1.16%\r",
      "Progress: 1.17%\r",
      "Progress: 1.18%\r",
      "Progress: 1.19%\r",
      "Progress: 1.20%\r",
      "Progress: 1.20%\r",
      "Progress: 1.21%\r",
      "Progress: 1.22%\r",
      "Progress: 1.23%\r",
      "Progress: 1.24%\r",
      "Progress: 1.24%\r",
      "Progress: 1.25%\r",
      "Progress: 1.26%\r",
      "Progress: 1.27%\r",
      "Progress: 1.28%\r",
      "Progress: 1.28%\r",
      "Progress: 1.29%\r",
      "Progress: 1.30%\r",
      "Progress: 1.31%\r",
      "Progress: 1.32%\r",
      "Progress: 1.32%\r",
      "Progress: 1.33%\r",
      "Progress: 1.34%\r",
      "Progress: 1.35%\r",
      "Progress: 1.36%\r",
      "Progress: 1.36%\r",
      "Progress: 1.37%\r",
      "Progress: 1.38%\r",
      "Progress: 1.39%\r",
      "Progress: 1.40%\r",
      "Progress: 1.40%\r",
      "Progress: 1.41%\r",
      "Progress: 1.42%\r",
      "Progress: 1.43%\r",
      "Progress: 1.44%\r",
      "Progress: 1.44%\r",
      "Progress: 1.45%\r",
      "Progress: 1.46%\r",
      "Progress: 1.47%\r",
      "Progress: 1.47%\r",
      "Progress: 1.48%\r",
      "Progress: 1.49%\r",
      "Progress: 1.50%\r",
      "Progress: 1.51%\r",
      "Progress: 1.51%\r",
      "Progress: 1.52%\r",
      "Progress: 1.53%\r",
      "Progress: 1.54%\r",
      "Progress: 1.55%\r",
      "Progress: 1.55%\r",
      "Progress: 1.56%\r",
      "Progress: 1.57%\r",
      "Progress: 1.58%\r",
      "Progress: 1.59%\r",
      "Progress: 1.59%\r",
      "Progress: 1.60%\r",
      "Progress: 1.61%\r",
      "Progress: 1.62%\r",
      "Progress: 1.63%\r",
      "Progress: 1.63%\r",
      "Progress: 1.64%\r",
      "Progress: 1.65%\r",
      "Progress: 1.66%\r",
      "Progress: 1.67%\r",
      "Progress: 1.67%\r",
      "Progress: 1.68%\r",
      "Progress: 1.69%\r",
      "Progress: 1.70%\r",
      "Progress: 1.71%\r",
      "Progress: 1.71%\r",
      "Progress: 1.72%\r",
      "Progress: 1.73%\r",
      "Progress: 1.74%\r",
      "Progress: 1.75%\r",
      "Progress: 1.75%\r",
      "Progress: 1.76%\r",
      "Progress: 1.77%\r",
      "Progress: 1.78%\r",
      "Progress: 1.79%\r",
      "Progress: 1.79%\r",
      "Progress: 1.80%\r",
      "Progress: 1.81%\r",
      "Progress: 1.82%\r",
      "Progress: 1.83%\r",
      "Progress: 1.83%\r",
      "Progress: 1.84%\r",
      "Progress: 1.85%\r",
      "Progress: 1.86%\r",
      "Progress: 1.87%\r",
      "Progress: 1.87%\r",
      "Progress: 1.88%\r",
      "Progress: 1.89%\r",
      "Progress: 1.90%\r",
      "Progress: 1.91%\r",
      "Progress: 1.91%\r",
      "Progress: 1.92%\r",
      "Progress: 1.93%\r",
      "Progress: 1.94%\r",
      "Progress: 1.95%\r",
      "Progress: 1.95%\r",
      "Progress: 1.96%\r",
      "Progress: 1.97%\r",
      "Progress: 1.98%\r",
      "Progress: 1.99%\r",
      "Progress: 1.99%\r",
      "Progress: 2.00%\r",
      "Progress: 2.01%\r",
      "Progress: 2.02%\r",
      "Progress: 2.03%\r",
      "Progress: 2.03%\r",
      "Progress: 2.04%\r",
      "Progress: 2.05%\r",
      "Progress: 2.06%\r",
      "Progress: 2.06%\r",
      "Progress: 2.07%\r",
      "Progress: 2.08%\r",
      "Progress: 2.09%\r",
      "Progress: 2.10%\r",
      "Progress: 2.10%\r",
      "Progress: 2.11%\r",
      "Progress: 2.12%\r",
      "Progress: 2.13%\r",
      "Progress: 2.14%\r",
      "Progress: 2.14%\r",
      "Progress: 2.15%\r",
      "Progress: 2.16%\r",
      "Progress: 2.17%\r",
      "Progress: 2.18%\r",
      "Progress: 2.18%\r",
      "Progress: 2.19%\r",
      "Progress: 2.20%\r",
      "Progress: 2.21%\r",
      "Progress: 2.22%\r",
      "Progress: 2.22%\r",
      "Progress: 2.23%\r",
      "Progress: 2.24%\r",
      "Progress: 2.25%\r",
      "Progress: 2.26%\r",
      "Progress: 2.26%\r",
      "Progress: 2.27%\r",
      "Progress: 2.28%\r",
      "Progress: 2.29%\r",
      "Progress: 2.30%\r",
      "Progress: 2.30%\r",
      "Progress: 2.31%\r",
      "Progress: 2.32%\r",
      "Progress: 2.33%\r",
      "Progress: 2.34%\r",
      "Progress: 2.34%\r",
      "Progress: 2.35%\r",
      "Progress: 2.36%\r",
      "Progress: 2.37%\r",
      "Progress: 2.38%\r",
      "Progress: 2.38%\r",
      "Progress: 2.39%\r",
      "Progress: 2.40%\r",
      "Progress: 2.41%\r",
      "Progress: 2.42%\r",
      "Progress: 2.42%\r",
      "Progress: 2.43%\r",
      "Progress: 2.44%\r",
      "Progress: 2.45%\r",
      "Progress: 2.46%\r",
      "Progress: 2.46%\r",
      "Progress: 2.47%\r",
      "Progress: 2.48%\r",
      "Progress: 2.49%\r",
      "Progress: 2.50%\r",
      "Progress: 2.50%\r",
      "Progress: 2.51%\r",
      "Progress: 2.52%\r",
      "Progress: 2.53%\r",
      "Progress: 2.54%\r",
      "Progress: 2.54%\r",
      "Progress: 2.55%\r",
      "Progress: 2.56%\r",
      "Progress: 2.57%\r",
      "Progress: 2.58%\r",
      "Progress: 2.58%\r",
      "Progress: 2.59%\r",
      "Progress: 2.60%\r",
      "Progress: 2.61%\r",
      "Progress: 2.62%\r",
      "Progress: 2.62%\r",
      "Progress: 2.63%\r",
      "Progress: 2.64%\r",
      "Progress: 2.65%\r",
      "Progress: 2.65%\r",
      "Progress: 2.66%\r",
      "Progress: 2.67%\r",
      "Progress: 2.68%\r",
      "Progress: 2.69%\r",
      "Progress: 2.69%\r",
      "Progress: 2.70%\r",
      "Progress: 2.71%\r",
      "Progress: 2.72%\r",
      "Progress: 2.73%\r",
      "Progress: 2.73%\r",
      "Progress: 2.74%\r",
      "Progress: 2.75%\r",
      "Progress: 2.76%\r",
      "Progress: 2.77%\r",
      "Progress: 2.77%\r",
      "Progress: 2.78%\r",
      "Progress: 2.79%\r",
      "Progress: 2.80%\r",
      "Progress: 2.81%\r",
      "Progress: 2.81%\r",
      "Progress: 2.82%\r",
      "Progress: 2.83%\r",
      "Progress: 2.84%\r",
      "Progress: 2.85%\r",
      "Progress: 2.85%\r",
      "Progress: 2.86%\r",
      "Progress: 2.87%\r",
      "Progress: 2.88%\r",
      "Progress: 2.89%\r",
      "Progress: 2.89%\r",
      "Progress: 2.90%\r",
      "Progress: 2.91%\r",
      "Progress: 2.92%\r",
      "Progress: 2.93%\r",
      "Progress: 2.93%\r",
      "Progress: 2.94%\r",
      "Progress: 2.95%\r",
      "Progress: 2.96%\r",
      "Progress: 2.97%\r",
      "Progress: 2.97%\r",
      "Progress: 2.98%\r",
      "Progress: 2.99%\r",
      "Progress: 3.00%\r",
      "Progress: 3.01%\r",
      "Progress: 3.01%\r",
      "Progress: 3.02%\r",
      "Progress: 3.03%\r",
      "Progress: 3.04%\r",
      "Progress: 3.05%\r",
      "Progress: 3.05%\r",
      "Progress: 3.06%\r",
      "Progress: 3.07%\r",
      "Progress: 3.08%\r",
      "Progress: 3.09%\r",
      "Progress: 3.09%\r",
      "Progress: 3.10%\r",
      "Progress: 3.11%\r",
      "Progress: 3.12%\r",
      "Progress: 3.13%\r",
      "Progress: 3.13%\r",
      "Progress: 3.14%\r",
      "Progress: 3.15%\r",
      "Progress: 3.16%\r",
      "Progress: 3.17%\r",
      "Progress: 3.17%\r",
      "Progress: 3.18%\r",
      "Progress: 3.19%\r",
      "Progress: 3.20%\r",
      "Progress: 3.20%\r",
      "Progress: 3.21%\r",
      "Progress: 3.22%\r",
      "Progress: 3.23%\r",
      "Progress: 3.24%\r",
      "Progress: 3.24%\r",
      "Progress: 3.25%\r",
      "Progress: 3.26%\r",
      "Progress: 3.27%\r",
      "Progress: 3.28%\r",
      "Progress: 3.28%\r",
      "Progress: 3.29%\r",
      "Progress: 3.30%\r",
      "Progress: 3.31%\r",
      "Progress: 3.32%\r",
      "Progress: 3.32%\r",
      "Progress: 3.33%\r",
      "Progress: 3.34%\r",
      "Progress: 3.35%\r",
      "Progress: 3.36%\r",
      "Progress: 3.36%\r",
      "Progress: 3.37%\r",
      "Progress: 3.38%\r",
      "Progress: 3.39%\r",
      "Progress: 3.40%\r",
      "Progress: 3.40%\r",
      "Progress: 3.41%\r",
      "Progress: 3.42%\r",
      "Progress: 3.43%\r",
      "Progress: 3.44%\r",
      "Progress: 3.44%\r",
      "Progress: 3.45%\r",
      "Progress: 3.46%\r",
      "Progress: 3.47%\r",
      "Progress: 3.48%\r",
      "Progress: 3.48%\r",
      "Progress: 3.49%\r",
      "Progress: 3.50%\r",
      "Progress: 3.51%\r",
      "Progress: 3.52%\r",
      "Progress: 3.52%\r",
      "Progress: 3.53%\r",
      "Progress: 3.54%\r",
      "Progress: 3.55%\r",
      "Progress: 3.56%\r",
      "Progress: 3.56%\r",
      "Progress: 3.57%\r",
      "Progress: 3.58%\r",
      "Progress: 3.59%\r",
      "Progress: 3.60%\r",
      "Progress: 3.60%\r",
      "Progress: 3.61%\r",
      "Progress: 3.62%\r",
      "Progress: 3.63%\r",
      "Progress: 3.64%\r",
      "Progress: 3.64%\r",
      "Progress: 3.65%\r",
      "Progress: 3.66%\r",
      "Progress: 3.67%\r",
      "Progress: 3.68%\r",
      "Progress: 3.68%\r",
      "Progress: 3.69%\r",
      "Progress: 3.70%\r",
      "Progress: 3.71%\r",
      "Progress: 3.72%\r",
      "Progress: 3.72%\r",
      "Progress: 3.73%\r",
      "Progress: 3.74%\r",
      "Progress: 3.75%\r",
      "Progress: 3.76%\r",
      "Progress: 3.76%\r",
      "Progress: 3.77%\r",
      "Progress: 3.78%\r",
      "Progress: 3.79%\r",
      "Progress: 3.79%\r",
      "Progress: 3.80%\r",
      "Progress: 3.81%\r",
      "Progress: 3.82%\r",
      "Progress: 3.83%\r",
      "Progress: 3.83%\r",
      "Progress: 3.84%\r",
      "Progress: 3.85%\r",
      "Progress: 3.86%\r",
      "Progress: 3.87%\r",
      "Progress: 3.87%\r",
      "Progress: 3.88%\r",
      "Progress: 3.89%\r",
      "Progress: 3.90%\r",
      "Progress: 3.91%\r",
      "Progress: 3.91%\r",
      "Progress: 3.92%\r",
      "Progress: 3.93%\r",
      "Progress: 3.94%\r",
      "Progress: 3.95%\r",
      "Progress: 3.95%\r",
      "Progress: 3.96%\r",
      "Progress: 3.97%\r",
      "Progress: 3.98%\r",
      "Progress: 3.99%\r",
      "Progress: 3.99%\r",
      "Progress: 4.00%\r",
      "Progress: 4.01%\r",
      "Progress: 4.02%\r",
      "Progress: 4.03%\r",
      "Progress: 4.03%\r",
      "Progress: 4.04%\r",
      "Progress: 4.05%\r",
      "Progress: 4.06%\r",
      "Progress: 4.07%\r",
      "Progress: 4.07%\r",
      "Progress: 4.08%\r",
      "Progress: 4.09%\r",
      "Progress: 4.10%\r",
      "Progress: 4.11%\r",
      "Progress: 4.11%\r",
      "Progress: 4.12%\r",
      "Progress: 4.13%\r",
      "Progress: 4.14%\r",
      "Progress: 4.15%\r",
      "Progress: 4.15%\r",
      "Progress: 4.16%\r",
      "Progress: 4.17%\r",
      "Progress: 4.18%\r",
      "Progress: 4.19%\r",
      "Progress: 4.19%\r",
      "Progress: 4.20%\r",
      "Progress: 4.21%\r",
      "Progress: 4.22%\r",
      "Progress: 4.23%\r",
      "Progress: 4.23%\r",
      "Progress: 4.24%\r",
      "Progress: 4.25%\r",
      "Progress: 4.26%\r",
      "Progress: 4.27%\r",
      "Progress: 4.27%\r",
      "Progress: 4.28%\r",
      "Progress: 4.29%\r",
      "Progress: 4.30%\r",
      "Progress: 4.31%\r",
      "Progress: 4.31%\r",
      "Progress: 4.32%\r",
      "Progress: 4.33%\r",
      "Progress: 4.34%\r",
      "Progress: 4.35%\r",
      "Progress: 4.35%\r",
      "Progress: 4.36%\r",
      "Progress: 4.37%\r",
      "Progress: 4.38%\r",
      "Progress: 4.38%\r",
      "Progress: 4.39%\r",
      "Progress: 4.40%\r",
      "Progress: 4.41%\r",
      "Progress: 4.42%\r",
      "Progress: 4.42%\r",
      "Progress: 4.43%\r",
      "Progress: 4.44%\r",
      "Progress: 4.45%\r",
      "Progress: 4.46%\r",
      "Progress: 4.46%\r",
      "Progress: 4.47%\r",
      "Progress: 4.48%\r",
      "Progress: 4.49%\r",
      "Progress: 4.50%\r",
      "Progress: 4.50%\r",
      "Progress: 4.51%\r",
      "Progress: 4.52%\r",
      "Progress: 4.53%\r",
      "Progress: 4.54%\r",
      "Progress: 4.54%\r",
      "Progress: 4.55%\r",
      "Progress: 4.56%\r",
      "Progress: 4.57%\r",
      "Progress: 4.58%\r",
      "Progress: 4.58%\r",
      "Progress: 4.59%\r",
      "Progress: 4.60%\r",
      "Progress: 4.61%\r",
      "Progress: 4.62%\r",
      "Progress: 4.62%\r",
      "Progress: 4.63%\r",
      "Progress: 4.64%\r",
      "Progress: 4.65%\r",
      "Progress: 4.66%\r",
      "Progress: 4.66%\r",
      "Progress: 4.67%\r",
      "Progress: 4.68%\r",
      "Progress: 4.69%\r",
      "Progress: 4.70%\r",
      "Progress: 4.70%\r",
      "Progress: 4.71%\r",
      "Progress: 4.72%\r",
      "Progress: 4.73%\r",
      "Progress: 4.74%\r",
      "Progress: 4.74%\r",
      "Progress: 4.75%\r",
      "Progress: 4.76%\r",
      "Progress: 4.77%\r",
      "Progress: 4.78%\r",
      "Progress: 4.78%\r",
      "Progress: 4.79%\r",
      "Progress: 4.80%\r",
      "Progress: 4.81%\r",
      "Progress: 4.82%\r",
      "Progress: 4.82%\r",
      "Progress: 4.83%\r",
      "Progress: 4.84%\r",
      "Progress: 4.85%\r",
      "Progress: 4.86%\r",
      "Progress: 4.86%\r",
      "Progress: 4.87%\r",
      "Progress: 4.88%\r",
      "Progress: 4.89%\r",
      "Progress: 4.90%\r",
      "Progress: 4.90%\r",
      "Progress: 4.91%\r",
      "Progress: 4.92%\r",
      "Progress: 4.93%\r",
      "Progress: 4.94%\r",
      "Progress: 4.94%\r",
      "Progress: 4.95%\r",
      "Progress: 4.96%\r",
      "Progress: 4.97%\r",
      "Progress: 4.97%\r",
      "Progress: 4.98%\r",
      "Progress: 4.99%\r",
      "Progress: 5.00%\r",
      "Progress: 5.01%\r",
      "Progress: 5.01%\r",
      "Progress: 5.02%\r",
      "Progress: 5.03%\r",
      "Progress: 5.04%\r",
      "Progress: 5.05%\r",
      "Progress: 5.05%\r",
      "Progress: 5.06%\r",
      "Progress: 5.07%\r",
      "Progress: 5.08%\r",
      "Progress: 5.09%\r",
      "Progress: 5.09%\r",
      "Progress: 5.10%\r",
      "Progress: 5.11%\r",
      "Progress: 5.12%\r",
      "Progress: 5.13%\r",
      "Progress: 5.13%\r",
      "Progress: 5.14%\r",
      "Progress: 5.15%\r",
      "Progress: 5.16%\r",
      "Progress: 5.17%\r",
      "Progress: 5.17%\r",
      "Progress: 5.18%\r",
      "Progress: 5.19%\r",
      "Progress: 5.20%\r",
      "Progress: 5.21%\r",
      "Progress: 5.21%\r",
      "Progress: 5.22%\r",
      "Progress: 5.23%\r",
      "Progress: 5.24%\r",
      "Progress: 5.25%\r",
      "Progress: 5.25%\r",
      "Progress: 5.26%\r",
      "Progress: 5.27%\r",
      "Progress: 5.28%\r",
      "Progress: 5.29%\r",
      "Progress: 5.29%\r",
      "Progress: 5.30%\r",
      "Progress: 5.31%\r",
      "Progress: 5.32%\r",
      "Progress: 5.33%\r",
      "Progress: 5.33%\r",
      "Progress: 5.34%\r",
      "Progress: 5.35%\r",
      "Progress: 5.36%\r",
      "Progress: 5.37%\r",
      "Progress: 5.37%\r",
      "Progress: 5.38%\r",
      "Progress: 5.39%\r",
      "Progress: 5.40%\r",
      "Progress: 5.41%\r",
      "Progress: 5.41%\r",
      "Progress: 5.42%\r",
      "Progress: 5.43%\r",
      "Progress: 5.44%\r",
      "Progress: 5.45%\r",
      "Progress: 5.45%\r",
      "Progress: 5.46%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 99.99%\n",
      "Accuracy: 83.09%\n"
     ]
    }
   ],
   "source": [
    "demo_tagger = DemoTagger()\n",
    "demo_tagger.train(training_data)\n",
    "print(\"Accuracy: {:.2%}\".format(nlp3.accuracy(demo_tagger, dev_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the features that are extracted when the system tags the sentence *Kim reads books*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: Kim reads books\n",
      "i: 0 (current word: Kim)\n",
      "pred_tags: \n",
      "features: Kim\n",
      "\n",
      "words: Kim reads books\n",
      "i: 1 (current word: reads)\n",
      "pred_tags: PROPN\n",
      "features: reads\n",
      "\n",
      "words: Kim reads books\n",
      "i: 2 (current word: books)\n",
      "pred_tags: PROPN VERB\n",
      "features: books\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['PROPN', 'VERB', 'NOUN']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_tagger.debug = True\n",
    "demo_tagger.tag(\"Kim reads books\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that a feature vector is represented as a list of Python values, as in lab&nbsp;L1. With this vector, the tagger then calls the perceptron to predict the next tag, and updates the configuration before moving on to the next word. Finally, `tag()` returns the list of predicted tags.\n",
    "\n",
    "### Training\n",
    "\n",
    "Training is based on the learning algorithm for the averaged perceptron. Note that the weight vectors need to be updated for each word, not for each sentence. The tagger maintains a list of already predicted tags as part of its configuration. The tagger trains for a single epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L3X: Feature engineering for part-of-speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the advanced part of this lab, you will practice your skills in **feature engineering**, the task of identifying useful features for a machine learning system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 2</div>\n",
    "<div class=\"panel-body\">\n",
    "<p>Think about which features could be useful for tagging and re-implement the method `features()` in the class `Tagger` accordingly. Provide a short description of how you came up with your features.</p>\n",
    "<p>The goal is to create a system whose accuracy on the development data is as high as possible. For a pass grade, you will have to achieve an accuracy of at least 87% on the development data.</p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "We suggest that you experiment not only with atomic features but also with different feature combinations (pairs or tuples of features).\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "You are not allowed to try re-engineering the reference system!\n",
    "</div>\n",
    "\n",
    "Note that the reference implementation uses integers to represent features; this is to make re-engineering slightly harder. (Internally, the reference implementation really uses tuples of key values.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the words we have made them **lowercase**, because we don't want to differentiate between the same word depending on if the word is in the beginning of the sentence or not.\n",
    "\n",
    "* **Current word** (lowercased): Self-evidently important\n",
    "* **Previous two tags**: Because this seems to give the model important context for a prediction of the current tag. We tried to add the previous two tags as separate features, but to add these as a single string gave a higher test accuracy. We also tried to only consider the previous tag, but adding the two previous resulted in a higher test set accuracy.\n",
    "* **Previous word** (lowercased): Similarly to n-grams this gives the model important context. We tried to add an additional previous word, but this resulted in a lower test set accuracy.  \n",
    "* **Next word** (lowercased): Same reasoning as why we added the previous word as a feature this gave the model additional context when considering the tag of the current word.\n",
    "\n",
    "**Accuracy: 88.39%** (we get higher accuracies when we train the model for more epochs.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
