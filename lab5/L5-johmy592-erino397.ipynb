{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**Due date:** 2018-02-23\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Semantic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **word embedding** represents words as vectors in a high-dimensional vector space. In this lab you will train word embeddings on the English Wikipedia via truncated singular value decomposition of the co-occurrence matrix.\n",
    "\n",
    "Start by loading the Python module for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp5\n",
    "import scipy\n",
    "import numpy as np\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this lab is the text of the [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page). We have excluded articles shorter than 50&nbsp;words, as well as certain meta-articles. The remaining articles were pre-processed by removing non-textual elements, sentence splitting, and tokenisation. The result is a text file containing 2.4M sentences, spanning 23M tokens.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Note that the data file is quite big. It is therefore very important to think about efficiency in this lab!\n",
    "</div>\n",
    "\n",
    "Because the data file is so big, we have compressed it using [bz2](https://en.wikipedia.org/wiki/Bzip2), which can be processed sequentially without completely decompressing the file. This functionality is provided by the `bz2` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (uncompressed) text contains one sentence per line, with individual tokens separated by spaces. To loop over the tokens, you can use the following generator function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(source):\n",
    "    for sentence in source:\n",
    "        for token in sentence.split():\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell shows you how you can open the compressed data file and print the number of tokens in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22963492\n"
     ]
    }
   ],
   "source": [
    "with bz2.open('/home/TDDE09/labs/l5/data/simplewiki.txt.bz2') as source:\n",
    "    print(sum(1 for t in tokens(source)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the use of the generator function to obtain the tokens is essential here &ndash; returning the tokens as a list would require a lot of memory. If you have not worked with generators and iterators before, now is a good time to read up on them. [More information about generators](https://wiki.python.org/moin/Generators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first task in this lab is to build the vocabulary of the word embedding that you are about to construct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 1</div>\n",
    "<div class=\"panel-body\">\n",
    "Write code that builds the vocabulary of the word embedding. Represent the vocabulary as a dictionary that maps words to a contiguous range of integer ids. Ignore words that occur less than 100&nbsp;times.\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "To solve this problem, complete the skeleton code in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(source, threshold=100):\n",
    "    # TODO: Replace the following line with your own code\n",
    "    V = {}\n",
    "    token_count = {}\n",
    "    for t in tokens(source):\n",
    "        token_count[t] = token_count.get(t, 0) + 1\n",
    "    i = 0\n",
    "    for t in token_count:\n",
    "        if t not in V and token_count[t] >= threshold:\n",
    "            V[t] = i\n",
    "            i+=1\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to help you test your implementation, we provide a smaller data file with the first 1M tokens from the full data. The code in the next cell builds the word-to-index mapping for this file and prints the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1256\n"
     ]
    }
   ],
   "source": [
    "with bz2.open('/home/TDDE09/labs/l5/data/simplewiki-small.txt.bz2') as source:\n",
    "    small_vocab = make_vocab(source)\n",
    "    print(len(small_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are confident that your implementation is correct, you can run it on the full data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14887\n"
     ]
    }
   ],
   "source": [
    "with bz2.open('/home/TDDE09/labs/l5/data/simplewiki.txt.bz2') as source:\n",
    "    vocab = make_vocab(source)\n",
    "    print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract context windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the co-occurrence matrix, we need to define the notion of &lsquo;context&rsquo;. Here we will use **linear contexts**, consisting of the words that precede and follow the target word in a window of $k$ tokens on each side. Your next task is to implement a generator function that extracts all such context windows from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 2</div>\n",
    "<div class=\"panel-body\">\n",
    "Implement a generator function that yields all context windows for the data. Represent context windows as tuples consisting of $2k+1$ tokens, with the target word in the center component of the tuple.\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "Later in the lab you will use context windows of width $k=2$, but your code should support any width $k \\geq 1$. The windows at the beginning and end of each sentence should be padded with `<bos>` and `<eos>` markers. With this padding, the total number of contexts should equal the number of tokens in the data.\n",
    "\n",
    "To solve the problem, complete the skeleton code in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contexts(source, k):\n",
    "    # TODO: Replace the following line with your own code\n",
    "    \n",
    "    for sentence in source:\n",
    "        \n",
    "        s_list = sentence.split()\n",
    "        for i in range(len(s_list)):\n",
    "            context = []\n",
    "            for pos in range(i-k,i+k+1):\n",
    "                if pos < 0: \n",
    "                    context.append('<bos>')\n",
    "                elif pos >= len(s_list):\n",
    "                    context.append('<eos>')\n",
    "                else:\n",
    "                    context.append(s_list[pos])\n",
    "            yield tuple(context)\n",
    "    #yield from nlp5.contexts(source, k)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your code, you can run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<bos>', '<bos>', b'april', b'is', b'the')\n",
      "('<bos>', b'april', b'is', b'the', b'th')\n",
      "(b'april', b'is', b'the', b'th', b'month')\n",
      "(b'is', b'the', b'th', b'month', b'of')\n",
      "(b'the', b'th', b'month', b'of', b'the')\n",
      "(b'th', b'month', b'of', b'the', b'year')\n",
      "(b'month', b'of', b'the', b'year', b'and')\n",
      "(b'of', b'the', b'year', b'and', b'comes')\n",
      "(b'the', b'year', b'and', b'comes', b'between')\n",
      "(b'year', b'and', b'comes', b'between', b'march')\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "with bz2.open('/home/TDDE09/labs/l5/data/simplewiki.txt.bz2') as source:\n",
    "    for context in islice(contexts(source, 2), 10):    # 10 windows of width k = 2\n",
    "        print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the co-occurrence matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next task is to construct the co-occurrence matrix for the data. However, rather than with raw counts, you will fill it with positive pointwise mutual information values.\n",
    "\n",
    "Recall that the **pointwise mutual information (PMI)** between a target word $w$ and a context word $c$ is defined as\n",
    "\n",
    "$$\n",
    "\\text{PMI}(w, c) = \\log \\Biggl( \\frac{\\#(w, c) \\cdot N}{\\#(w) \\cdot \\#(c)} \\Biggr)\n",
    "$$\n",
    "\n",
    "where $\\#(w, c)$ is the number of times $w$ was observed in the same context as $c$, $\\#(w)$ is the total number of times $w$ was observed, $\\#(c)$ is the total number of times $c$ was observed, and $N$ is the total number of observations. In the case where either the enumerator or the denominator of this expression is zero, we let $\\text{PMI}(w, c) = 0$.\n",
    "\n",
    "**Positive pointwise mutual information (PPMI)** is derived from PMI by clipping all negative values:\n",
    "\n",
    "$$\n",
    "\\text{PPMI}(w, c) = \\max \\bigl(0, \\text{PMI}(w, c) \\bigr)\n",
    "$$\n",
    "\n",
    "Here we will actually use a shifted version of PPMI, where the PMI value is decreased by a constant $\\log \\delta$ before clipping. For $\\delta = 1$, this gives the same result as standard PMI. Higher values of $\\delta$ can improve the performance of word embeddings for different tasks ([Levy and Goldberg, 2014](https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization/)).\n",
    "\n",
    "$$\n",
    "\\text{PPMI}(w, c) = \\max \\bigl(0, \\text{PMI}(w, c) - \\log \\delta \\bigr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 3</div>\n",
    "<div class=\"panel-body\">\n",
    "Write code that builds the shifted PPMI co-occurrence matrix for the data. Represent it as a [SciPy sparse matrix](https://docs.scipy.org/doc/scipy/reference/sparse.html) whose row indices correspond to the target words, and whose column indices correspond to the context words.\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "To solve this problem, complete the skeleton code in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ppmi_matrix(vocab, source, k=2, delta=1.0):\n",
    "    # TODO: Replace the following line with your own code\n",
    "    contexts_list = []\n",
    "    for context in contexts(list(source),k):\n",
    "        contexts_list.append(context)\n",
    "    context_matrix = np.zeros((len(vocab),len(vocab)))\n",
    "    N = 0\n",
    "    for ctxt in contexts_list:\n",
    "        word = ctxt[k]\n",
    "        if word not in vocab:\n",
    "            continue\n",
    "        context_words = [w for w in ctxt if w != word and w in vocab]\n",
    "        N += len(context_words)\n",
    "                         \n",
    "        for w in context_words:\n",
    "            context_matrix[vocab[word], vocab[w]] += 1\n",
    "\n",
    "    w_count = np.sum(context_matrix, axis=1, keepdims=True)\n",
    "    c_count = np.sum(context_matrix, axis=0, keepdims=True)\n",
    "    \n",
    "    norm_cm = context_matrix * N / w_count.dot(c_count)\n",
    "\n",
    "    # Set zero valued elements to 1 (log(1) == 0)\n",
    "    norm_cm[norm_cm == 0] = 1\n",
    "    \n",
    "    log_norm_cm = np.log(norm_cm) - np.log(delta)\n",
    "    \n",
    "    # Set negative values to zero\n",
    "    log_norm_cm[log_norm_cm < 0] = 0\n",
    "    \n",
    "    return scipy.sparse.csr_matrix(log_norm_cm)\n",
    "\n",
    "def ref_ppmi_matrix(vocab, source, k=2, delta=1.0):\n",
    "    return nlp5.make_ppmi_matrix(vocab, source, k, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the problem of constructing the shifted PPMI matrix is not hard from a conceptual point of view, writing efficient code for it is slightly harder. We recommend to proceed in two steps: First, collect the relevant counts $\\#(w, c)$, $\\#(w)$, $\\#(c)$, and $N$ in standard Python data structures. Then, use these counts to compute the shifted PPMI values and return them as a matrix in [CSR format](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html). (See the documentation of scipy.sparse.csr_matrix for an example of how to do this.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test your code by running it on the small data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ours: 5.356584310531616\n",
      "Processed 1000000 contexts\n",
      "Computed 1256 word vectors\n",
      "Reference: 5.4228174686431885\n"
     ]
    }
   ],
   "source": [
    "with bz2.open('/home/TDDE09/labs/l5/data/simplewiki-small.txt.bz2') as source:\n",
    "    #print(list(source)[0])\n",
    "    small_vocab = make_vocab(source)\n",
    "    \n",
    "now = time.time()\n",
    "with bz2.open('/home/TDDE09/labs/l5/data/simplewiki-small.txt.bz2') as source:\n",
    "    #print(list(source)[0])\n",
    "    ppmi_matrix = make_ppmi_matrix(small_vocab, source)\n",
    "\n",
    "end = time.time()\n",
    "print (\"Ours: %s\" % (end - now))\n",
    " \n",
    "now = time.time()\n",
    "with bz2.open('/home/TDDE09/labs/l5/data/simplewiki-small.txt.bz2') as source:\n",
    "    #print(list(source)[0])\n",
    "    ppmi_matrix = ref_ppmi_matrix(small_vocab, source)\n",
    "    \n",
    "end = time.time()\n",
    "print (\"Reference: %s\" % (end - now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You should now be able to obtain the PPMI as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.952847218567449"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppmi_matrix[small_vocab[b'april'], small_vocab[b'december']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you feel confident that your code is correct, you can run it on the full data. (This will take a while.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open('/home/TDDE09/labs/l5/data/simplewiki.txt.bz2') as source:\n",
    "    ppmi_matrix = make_ppmi_matrix(vocab, source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid re-computing the matrix several times, you can save it to a file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "key b'mix' is not a string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-34cbb24b7f48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vocab.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# ppmi_matrix = scipy.sparse.load_npz('simplewiki.npz')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.4/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" is not a string\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: key b'mix' is not a string"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "scipy.sparse.save_npz('simplewiki.npz', ppmi_matrix)\n",
    "\n",
    "#with open('vocab.json', 'w') as json_file:\n",
    "#    json.dump(vocab, json_file)\n",
    "# ppmi_matrix = scipy.sparse.load_npz('simplewiki.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the PPMI matrix will require approximately 60&nbsp;MB of disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the PPMI matrix, you can construct the word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 4</div>\n",
    "<div class=\"panel-body\">\n",
    "Implement a class representing word embeddings. The class should support the construction of the embedding by means of truncated singular value decomposition.\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "More specifically, we ask you to implement the following interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "class Embedding(object):\n",
    "    \"\"\"A word embedding.\n",
    "\n",
    "    A word embedding represents words as vectors in a d-dimensional\n",
    "    vector space. The central attribute of the word embedding is a\n",
    "    dense matrix whose rows correspond to the words in some\n",
    "    vocabulary, and whose columns correspond to the d dimensions in\n",
    "    the embedding space.\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "        vocab: The vocabulary, specified as a dictionary that maps\n",
    "            words to integer indices, identifying rows in the embedding\n",
    "            matrix.\n",
    "        dim: The dimensionality of the word vectors.\n",
    "        m: The embedding matrix. The rows of this matrix correspond to the\n",
    "            words in the vocabulary.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab, matrix, dim=100):\n",
    "        \"\"\"Initialies a new word embedding.\n",
    "\n",
    "        Args:\n",
    "            vocab: The vocabulary for the embedding, specified as a\n",
    "                dictionary that maps words to integer indices.\n",
    "            matrix: The co-occurrence matrix, represented as a SciPy\n",
    "                sparse matrix with one row and one column for each word in\n",
    "                the vocabulary.\n",
    "            dim: The dimensionality of the word vectors.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.dim = dim\n",
    "        self.svd = TruncatedSVD(n_components=dim,n_iter=10)\n",
    "        self.m = self.svd.fit_transform(matrix)\n",
    "\n",
    "    def vec(self, w):\n",
    "        \"\"\"Returns the vector for the specified word.\n",
    "\n",
    "        Args:\n",
    "            w: A word, an element of the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            The word vector for the specified word.\n",
    "        \"\"\"\n",
    "        # TODO: Replace the following line with your own code\n",
    "        return self.m[self.vocab[w]]\n",
    "\n",
    "    def distance(self, w1, w2):\n",
    "        \"\"\"Computes the cosine similarity between the specified words.\n",
    "        \n",
    "        Args:\n",
    "            w1: The first word (an element of the vocabulary).\n",
    "            w2: The second word (an element of the vocabulary).\n",
    "        \n",
    "        Returns:\n",
    "            The cosine similarity between the specified words in this\n",
    "            embedding.\n",
    "        \"\"\"\n",
    "        # So we can compare vectors with words in analogy()\n",
    "        if type(w1).__module__ == \"numpy\":\n",
    "            v1 = w1\n",
    "        else:\n",
    "            v1 = self.vec(w1)\n",
    "        v2 = self.vec(w2)\n",
    "        \n",
    "        return v1.dot(v2.T) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    \n",
    "    def most_similar(self, w, n=10):\n",
    "        \"\"\"Returns the most similar words for the specified word.\n",
    "\n",
    "        Args:\n",
    "            w: A word, an element of the vocabulary.\n",
    "            n: The maximal number of most similar words to return.\n",
    "\n",
    "        Returns:\n",
    "            A list containing distance/word pairs.\n",
    "        \"\"\"\n",
    "        words = list(self.vocab.keys())\n",
    "        distances = [self.distance(w, w_) for w_ in words]\n",
    "        similarity_indices = np.argsort(distances)[::-1]\n",
    "        similar_words = [words[i] for i in similarity_indices[:n]]\n",
    "        return similar_words\n",
    "\n",
    "    def analogy(self, w1, w2, w3):\n",
    "        \"\"\"Answers an analogy question of the form w1 - w2 + w3 = ?\n",
    "\n",
    "        Args:\n",
    "            w1: A word, an element of the vocabulary.\n",
    "            w2: A word, an element of the vocabulary.\n",
    "            w3: A word, an element of the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            The word closest to the vector w1 - w2 + w3 that is different\n",
    "            from all the other words.\n",
    "        \"\"\"\n",
    "        v1 = self.vec(w1)\n",
    "        v2 = self.vec(w2)\n",
    "        v3 = self.vec(w3)\n",
    "        \n",
    "        v = v1 - v2 + v3\n",
    "        \n",
    "        most_sim = self.most_similar(v, n=4)\n",
    "        for w in most_sim:\n",
    "            if w not in [w1,w2,w3]:\n",
    "                return w\n",
    "        \n",
    "        #norm_context_matrixnorm_context_matrixnorm_context_matrixreturn most_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the **singular value decomposition** of an $m \\times n$ matrix $\\mathbf{M}$ is a factorization of the form $\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^*$ where $\\mathbf{U}$ is an $m \\times m$ matrix of which we may assume that its columns are sorted in decreasing order of importance when it comes to explaining the variance of $\\mathbf{M}$. (Formally, these columns correspond to the singular values in the matrix $\\mathbf{\\Sigma}$.) By truncating $\\mathbf{U}$ after the first $\\mathit{dim}$ columns, we thus obtain an approximation of the original matrix $\\mathbf{M}$. In your case, $\\mathbf{M}$ is the PPMI matrix, and the truncated matrix $\\mathbf{U}$ gives the word vectors of the embedding. To compute the matrix $\\mathbf{U}$, you can use the class [TruncatedSVD](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html), which is available in [scikit-learn](http://scikit-learn.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows how to initalise a new embedding with the PPMI matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14887, 14887)\n"
     ]
    }
   ],
   "source": [
    "print (ppmi_matrix.shape)\n",
    "embedding = Embedding(vocab, ppmi_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some things that you can do with the embedding:\n",
    "\n",
    "#### Word similarity\n",
    "\n",
    "What is the semantic similarity between &lsquo;man&rsquo; and &lsquo;woman&rsquo;?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8322261392443765\n",
      "0.6748007883787293\n",
      "0.2653145159961847\n"
     ]
    }
   ],
   "source": [
    "print (embedding.distance(b'man', b'woman'))\n",
    "print (embedding.distance(b'man', b'guy'))\n",
    "print (embedding.distance(b'man', b'banana'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What words are most similar to &lsquo;man&rsquo;?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'man', b'boy', b'woman', b'girl', b'dog', b'little', b'dead', b'young', b'child', b'who']\n"
     ]
    }
   ],
   "source": [
    "print (embedding.most_similar(b'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What words are most similar to &lsquo;woman&rsquo;?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'woman', b'child', b'man', b'girl', b'herself', b'her', b'person', b'girls', b'children', b'boy']\n"
     ]
    }
   ],
   "source": [
    "print (embedding.most_similar(b'woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analogies\n",
    "\n",
    "Here is the famous king &minus; man + woman = ? example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'heir'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.analogy(b'king', b'man', b'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When experimenting with other examples, you will find that the embedding picks up common stereotypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'nurse'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.analogy(b'doctor', b'man', b'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model knows the capital of Sweden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'stockholm'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.analogy(b'berlin', b'germany', b'sweden')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding also &lsquo;learns&rsquo; some syntactic analogies, such as the analogy between the past-tense and present-tense forms of verbs (here: *jump* and *eat*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'feed'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.analogy(b'jumped', b'jump', b'eat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
