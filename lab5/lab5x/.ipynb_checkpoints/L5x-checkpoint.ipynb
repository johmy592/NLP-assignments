{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**Due date:** 2018-02-23\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Semantic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **word embedding** represents words as vectors in a high-dimensional vector space. In this lab you will train word embeddings on the English Wikipedia via truncated singular value decomposition of the co-occurrence matrix.\n",
    "\n",
    "Start by loading the Python module for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp5\n",
    "import scipy\n",
    "import numpy as np\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(source):\n",
    "    for sentence in source:\n",
    "        for token in sentence.split():\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the use of the generator function to obtain the tokens is essential here &ndash; returning the tokens as a list would require a lot of memory. If you have not worked with generators and iterators before, now is a good time to read up on them. [More information about generators](https://wiki.python.org/moin/Generators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(source, threshold=100):\n",
    "    # TODO: Replace the following line with your own code\n",
    "    V = {}\n",
    "    token_count = {}\n",
    "    for t in tokens(source):\n",
    "        token_count[t] = token_count.get(t, 0) + 1\n",
    "    i = 0\n",
    "    for t in token_count:\n",
    "        if t not in V and token_count[t] >= threshold:\n",
    "            V[t] = i\n",
    "            i+=1\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9313\n"
     ]
    }
   ],
   "source": [
    "with bz2.open('/home/TDDE09/labs/l5x/data/oanc.txt.bz2') as source:\n",
    "    vocab = make_vocab(source)\n",
    "    print(len(vocab))\n",
    "    #print(vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to help you test your implementation, we provide a smaller data file with the first 1M tokens from the full data. The code in the next cell builds the word-to-index mapping for this file and prints the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contexts(source, k):\n",
    "    # TODO: Replace the following line with your own code\n",
    "    \n",
    "    for sentence in source:\n",
    "        \n",
    "        s_list = sentence.split()\n",
    "        for i in range(len(s_list)):\n",
    "            context = []\n",
    "            for pos in range(i-k,i+k+1):\n",
    "                if pos < 0: \n",
    "                    context.append('<bos>')\n",
    "                elif pos >= len(s_list):\n",
    "                    context.append('<eos>')\n",
    "                else:\n",
    "                    context.append(s_list[pos])\n",
    "            yield tuple(context)\n",
    "    #yield from nlp5.contexts(source, k)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your code, you can run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ppmi_matrix(vocab, source, k=2, delta=1.0):\n",
    "    # TODO: Replace the following line with your own code\n",
    "    contexts_list = []\n",
    "    for context in contexts(list(source),k):\n",
    "        contexts_list.append(context)\n",
    "    context_matrix = np.zeros((len(vocab),len(vocab)))\n",
    "    N = 0\n",
    "    for ctxt in contexts_list:\n",
    "        word = ctxt[k]\n",
    "        if word not in vocab:\n",
    "            continue\n",
    "        context_words = [w for w in ctxt if w != word and w in vocab]\n",
    "        N += len(context_words)\n",
    "                         \n",
    "        for w in context_words:\n",
    "            context_matrix[vocab[word], vocab[w]] += 1\n",
    "\n",
    "    w_count = np.sum(context_matrix, axis=1, keepdims=True)\n",
    "    c_count = np.sum(context_matrix, axis=0, keepdims=True)\n",
    "    \n",
    "    norm_cm = context_matrix * N / w_count.dot(c_count)\n",
    "\n",
    "    # Set zero valued elements to 1 (log(1) == 0)\n",
    "    norm_cm[norm_cm == 0] = 1\n",
    "    \n",
    "    log_norm_cm = np.log(norm_cm) - np.log(delta)\n",
    "    \n",
    "    # Set negative values to zero\n",
    "    log_norm_cm[log_norm_cm < 0] = 0\n",
    "    \n",
    "    return scipy.sparse.csr_matrix(log_norm_cm)\n",
    "\n",
    "def ref_ppmi_matrix(vocab, source, k=2, delta=1.0):\n",
    "    return nlp5.make_ppmi_matrix(vocab, source, k, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open('/home/TDDE09/labs/l5x/data/oanc.txt.bz2') as source:\n",
    "    ppmi_matrix = make_ppmi_matrix(vocab, source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You should now be able to obtain the PPMI as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "scipy.sparse.save_npz('simplewnorm_context_matrixiki.npz', ppmi_matrix)\n",
    "#ppmi_matrix = scipy.sparse.load_npz('simplewnorm_context_matrixiki.npz')\n",
    "\n",
    "#with open('vocab.json') as json_file:\n",
    "#    vocab = json.loads(json_file.read())\n",
    "    \n",
    "#print (vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the PPMI matrix will require approximately 60&nbsp;MB of disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the PPMI matrix, you can construct the word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 4</div>\n",
    "<div class=\"panel-body\">\n",
    "Implement a class representing word embeddings. The class should support the construction of the embedding by means of truncated singular value decomposition.\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "More specifically, we ask you to implement the following interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "class Embedding(object):\n",
    "    \"\"\"A word embedding.\n",
    "\n",
    "    A word embedding represents words as vectors in a d-dimensional\n",
    "    vector space. The central attribute of the word embedding is a\n",
    "    dense matrix whose rows correspond to the words in some\n",
    "    vocabulary, and whose columns correspond to the d dimensions in\n",
    "    the embedding space.\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "        vocab: The vocabulary, specified as a dictionary that maps\n",
    "            words to integer indices, identifying rows in the embedding\n",
    "            matrix.\n",
    "        dim: The dimensionality of the word vectors.\n",
    "        m: The embedding matrix. The rows of this matrix correspond to the\n",
    "            words in the vocabulary.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab, matrix, dim=100):\n",
    "        \"\"\"Initialies a new word embedding.\n",
    "\n",
    "        Args:\n",
    "            vocab: The vocabulary for the embedding, specified as a\n",
    "                dictionary that maps words to integer indices.\n",
    "            matrix: The co-occurrence matrix, represented as a SciPy\n",
    "                sparse matrix with one row and one column for each word in\n",
    "                the vocabulary.\n",
    "            dim: The dimensionality of the word vectors.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.dim = dim\n",
    "        self.svd = TruncatedSVD(n_components=dim)\n",
    "        self.m = self.svd.fit_transform(matrix)\n",
    "\n",
    "    def vec(self, w):\n",
    "        \"\"\"Returns the vector for the specified word.\n",
    "\n",
    "        Args:\n",
    "            w: A word, an element of the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            The word vector for the specified word.\n",
    "        \"\"\"\n",
    "        # TODO: Replace the following line with your own code\n",
    "        return self.m[self.vocab[w]]\n",
    "\n",
    "    def distance(self, w1, w2):\n",
    "        \"\"\"Computes the cosine similarity between the specified words.\n",
    "        \n",
    "        Args:\n",
    "            w1: The first word (an element of the vocabulary).\n",
    "            w2: The second word (an element of the vocabulary).\n",
    "        \n",
    "        Returns:\n",
    "            The cosine similarity between the specified words in this\n",
    "            embedding.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # So we can compare vectors with words in analogy()\n",
    "            if type(w1).__module__ == \"numpy\":\n",
    "                v1 = w1\n",
    "            else:\n",
    "                v1 = self.vec(w1)\n",
    "            v2 = self.vec(w2)\n",
    "\n",
    "            return v1.dot(v2.T) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        except:\n",
    "            print(w1,\" \", str(w1 in self.vocab))\n",
    "            print(w2,\" \", str(w2 in self.vocab))\n",
    "            return 0\n",
    "    \n",
    "    def most_similar(self, w, n=10):\n",
    "        \"\"\"Returns the most similar words for the specified word.\n",
    "\n",
    "        Args:\n",
    "            w: A word, an element of the vocabulary.\n",
    "            n: The maximal number of most similar words to return.\n",
    "\n",
    "        Returns:\n",
    "            A list containing distance/word pairs.\n",
    "        \"\"\"\n",
    "        words = list(self.vocab.keys())\n",
    "        distances = [self.distance(w, w_) for w_ in words]\n",
    "        similarity_indices = np.argsort(distances)[::-1]\n",
    "        similar_words = [words[i] for i in similarity_indices[:n]]\n",
    "        return similar_words\n",
    "\n",
    "    def analogy(self, w1, w2, w3):\n",
    "        \"\"\"Answers an analogy question of the form w1 - w2 + w3 = ?\n",
    "\n",
    "        Args:\n",
    "            w1: A word, an element of the vocabulary.\n",
    "            w2: A word, an element of the vocabulary.\n",
    "            w3: A word, an element of the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            The word closest to the vector w1 - w2 + w3 that is different\n",
    "            from all the other words.\n",
    "        \"\"\"\n",
    "        v1 = self.vec(w1)\n",
    "        v2 = self.vec(w2)\n",
    "        v3 = self.vec(w3)\n",
    "        \n",
    "        v = v1 - v2 + v3\n",
    "        \n",
    "        most_sim = self.most_similar(v, n=4)\n",
    "        for w in most_sim:\n",
    "            if w not in [w1,w2,w3]:\n",
    "                return w\n",
    "        \n",
    "        #norm_context_matrixnorm_context_matrixnorm_context_matrixreturn most_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the **singular value decomposition** of an $m \\times n$ matrix $\\mathbf{M}$ is a factorization of the form $\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^*$ where $\\mathbf{U}$ is an $m \\times m$ matrix of which we may assume that its columns are sorted in decreasing order of importance when it comes to explaining the variance of $\\mathbf{M}$. (Formally, these columns correspond to the singular values in the matrix $\\mathbf{\\Sigma}$.) By truncating $\\mathbf{U}$ after the first $\\mathit{dim}$ columns, we thus obtain an approximation of the original matrix $\\mathbf{M}$. In your case, $\\mathbf{M}$ is the PPMI matrix, and the truncated matrix $\\mathbf{U}$ gives the word vectors of the embedding. To compute the matrix $\\mathbf{U}$, you can use the class [TruncatedSVD](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html), which is available in [scikit-learn](http://scikit-learn.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows how to initalise a new embedding with the PPMI matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9313, 9313)\n"
     ]
    }
   ],
   "source": [
    "print (ppmi_matrix.shape)\n",
    "embedding = Embedding(vocab, ppmi_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some things that you can do with the embedding:\n",
    "\n",
    "#### Word similarity\n",
    "\n",
    "What is the semantic similarity between &lsquo;man&rsquo; and &lsquo;woman&rsquo;?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9175105702081308\n",
      "0.8281382751906086\n",
      "b'man'   True\n",
      "b'banana'   False\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print (embedding.distance(b'man', b'woman'))\n",
    "print (embedding.distance(b'man', b'guy'))\n",
    "print (embedding.distance(b'man', b'banana'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What words are most similar to &lsquo;man&rsquo;?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'man', b'woman', b'boy', b'girl', b'person', b'guy', b'father', b'lady', b'mother', b'wife']\n"
     ]
    }
   ],
   "source": [
    "print (embedding.most_similar(b'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What words are most similar to &lsquo;woman&rsquo;?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'woman', b'man', b'girl', b'boy', b'person', b'lady', b'mother', b'baby', b'father', b'wife']\n"
     ]
    }
   ],
   "source": [
    "print (embedding.most_similar(b'woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analogies\n",
    "\n",
    "Here is the famous king &minus; man + woman = ? example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'prince'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.analogy(b'king', b'man', b'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When experimenting with other examples, you will find that the embedding picks up common stereotypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'nurse'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.analogy(b'doctor', b'man', b'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model knows the capital of Sweden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "b'sweden'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-a61d0dba9c44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'berlin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb'germany'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb'sweden'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-af8f14a9fb39>\u001b[0m in \u001b[0;36manalogy\u001b[0;34m(self, w1, w2, w3)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mv3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mv3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-af8f14a9fb39>\u001b[0m in \u001b[0;36mvec\u001b[0;34m(self, w)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \"\"\"\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# TODO: Replace the following line with your own code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: b'sweden'"
     ]
    }
   ],
   "source": [
    "#embedding.analogy(b'berlin', b'germany', b'sweden')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding also &lsquo;learns&rsquo; some syntactic analogies, such as the analogy between the past-tense and present-tense forms of verbs (here: *jump* and *eat*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding.analogy(b'jumped', b'jump', b'eat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'enormously'   False\n",
      "b'appropriately'   True\n",
      "b'enormously'   False\n",
      "b'uniquely'   False\n",
      "b'enormously'   False\n",
      "b'tremendously'   False\n",
      "b'enormously'   False\n",
      "b'decidedly'   False\n",
      "b'provisions'   True\n",
      "b'stipulations'   False\n",
      "b'provisions'   True\n",
      "b'interrelations'   False\n",
      "b'provisions'   True\n",
      "b'jurisdictions'   False\n",
      "b'provisions'   True\n",
      "b'interpretations'   False\n",
      "b'haphazardly'   False\n",
      "b'dangerously'   False\n",
      "b'haphazardly'   False\n",
      "b'densely'   False\n",
      "b'haphazardly'   False\n",
      "b'randomly'   True\n",
      "b'haphazardly'   False\n",
      "b'linearly'   False\n",
      "b'prominent'   True\n",
      "b'battered'   False\n",
      "b'prominent'   True\n",
      "b'conspicuous'   False\n",
      "b'zenith'   False\n",
      "b'completion'   True\n",
      "b'zenith'   False\n",
      "b'pinnacle'   False\n",
      "b'zenith'   False\n",
      "b'outset'   False\n",
      "b'zenith'   False\n",
      "b'decline'   True\n",
      "b'flawed'   False\n",
      "b'tiny'   True\n",
      "b'flawed'   False\n",
      "b'imperfect'   False\n",
      "b'flawed'   False\n",
      "b'lustrous'   False\n",
      "b'flawed'   False\n",
      "b'crude'   True\n",
      "b'urgently'   False\n",
      "b'typically'   True\n",
      "b'urgently'   False\n",
      "b'conceivably'   False\n",
      "b'urgently'   False\n",
      "b'tentatively'   False\n",
      "b'urgently'   False\n",
      "b'desperately'   True\n",
      "b'consumed'   True\n",
      "b'bred'   False\n",
      "b'consumed'   True\n",
      "b'eaten'   False\n",
      "b'concisely'   False\n",
      "b'powerfully'   False\n",
      "b'concisely'   False\n",
      "b'positively'   True\n",
      "b'concisely'   False\n",
      "b'freely'   True\n",
      "b'concisely'   False\n",
      "b'succinctly'   False\n",
      "b'salutes'   False\n",
      "b'information'   True\n",
      "b'salutes'   False\n",
      "b'ceremonies'   False\n",
      "b'salutes'   False\n",
      "b'greetings'   False\n",
      "b'salutes'   False\n",
      "b'privileges'   False\n",
      "b'solitary'   False\n",
      "b'alert'   True\n",
      "b'solitary'   False\n",
      "b'restless'   False\n",
      "b'solitary'   False\n",
      "b'alone'   True\n",
      "b'solitary'   False\n",
      "b'fearless'   False\n",
      "b'hasten'   False\n",
      "b'permit'   True\n",
      "b'hasten'   False\n",
      "b'determine'   True\n",
      "b'hasten'   False\n",
      "b'accelerate'   False\n",
      "b'hasten'   False\n",
      "b'accompany'   False\n",
      "b'perseverance'   False\n",
      "b'endurance'   False\n",
      "b'perseverance'   False\n",
      "b'skill'   True\n",
      "b'perseverance'   False\n",
      "b'generosity'   False\n",
      "b'perseverance'   False\n",
      "b'disturbance'   False\n",
      "b'fanciful'   False\n",
      "b'familiar'   True\n",
      "b'fanciful'   False\n",
      "b'imaginative'   True\n",
      "b'fanciful'   False\n",
      "b'apparent'   True\n",
      "b'fanciful'   False\n",
      "b'logical'   True\n",
      "b'showed'   True\n",
      "b'postponed'   False\n",
      "b'constantly'   True\n",
      "b'instantly'   False\n",
      "b'constantly'   True\n",
      "b'accidentally'   False\n",
      "b'furnish'   False\n",
      "b'supply'   True\n",
      "b'furnish'   False\n",
      "b'impress'   False\n",
      "b'furnish'   False\n",
      "b'protect'   True\n",
      "b'furnish'   False\n",
      "b'advise'   True\n",
      "b'spot'   True\n",
      "b'latitude'   False\n",
      "b'make'   True\n",
      "b'borrow'   False\n",
      "b'often'   True\n",
      "b'chemically'   False\n",
      "b'easygoing'   False\n",
      "b'frontier'   False\n",
      "b'easygoing'   False\n",
      "b'boring'   True\n",
      "b'easygoing'   False\n",
      "b'farming'   False\n",
      "b'easygoing'   False\n",
      "b'relaxed'   True\n",
      "b'narrow'   True\n",
      "b'freezing'   False\n",
      "b'narrow'   True\n",
      "b'poisonous'   False\n",
      "b'infinite'   True\n",
      "b'limitless'   False\n",
      "b'showy'   False\n",
      "b'striking'   True\n",
      "b'showy'   False\n",
      "b'prickly'   False\n",
      "b'showy'   False\n",
      "b'entertaining'   True\n",
      "b'showy'   False\n",
      "b'incidental'   False\n",
      "b'levied'   False\n",
      "b'imposed'   True\n",
      "b'levied'   False\n",
      "b'believed'   True\n",
      "b'levied'   False\n",
      "b'requested'   True\n",
      "b'levied'   False\n",
      "b'correlated'   True\n",
      "b'deftly'   False\n",
      "b'skillfully'   False\n",
      "b'deftly'   False\n",
      "b'prudently'   False\n",
      "b'deftly'   False\n",
      "b'occasionally'   True\n",
      "b'deftly'   False\n",
      "b'humorously'   False\n",
      "b'distribute'   False\n",
      "b'commercialize'   False\n",
      "b'distribute'   False\n",
      "b'circulate'   False\n",
      "b'distribute'   False\n",
      "b'research'   True\n",
      "b'distribute'   False\n",
      "b'acknowledge'   True\n",
      "b'discrepancies'   False\n",
      "b'weights'   True\n",
      "b'discrepancies'   False\n",
      "b'deposits'   False\n",
      "b'discrepancies'   False\n",
      "b'wavelengths'   False\n",
      "b'discrepancies'   False\n",
      "b'differences'   True\n",
      "b'prolific'   False\n",
      "b'productive'   True\n",
      "b'prolific'   False\n",
      "b'serious'   True\n",
      "b'prolific'   False\n",
      "b'capable'   True\n",
      "b'prolific'   False\n",
      "b'promising'   True\n",
      "b'unmatched'   False\n",
      "b'unrecognized'   False\n",
      "b'unmatched'   False\n",
      "b'unequaled'   False\n",
      "b'unmatched'   False\n",
      "b'alienated'   False\n",
      "b'unmatched'   False\n",
      "b'emulated'   False\n",
      "b'peculiarly'   False\n",
      "b'partly'   True\n",
      "b'peculiarly'   False\n",
      "b'uniquely'   False\n",
      "b'peculiarly'   False\n",
      "b'patriotically'   False\n",
      "b'peculiarly'   False\n",
      "b'suspiciously'   False\n",
      "b'hue'   False\n",
      "b'glare'   False\n",
      "b'hue'   False\n",
      "b'contrast'   True\n",
      "b'hue'   False\n",
      "b'color'   True\n",
      "b'hue'   False\n",
      "b'scent'   False\n",
      "b'hind'   False\n",
      "b'curved'   False\n",
      "b'hind'   False\n",
      "b'muscular'   False\n",
      "b'hind'   False\n",
      "b'hairy'   False\n",
      "b'hind'   False\n",
      "b'rear'   True\n",
      "b'highlight'   True\n",
      "b'imitate'   False\n",
      "b'highlight'   True\n",
      "b'accentuate'   False\n",
      "b'hastily'   False\n",
      "b'hurriedly'   False\n",
      "b'hastily'   False\n",
      "b'shrewdly'   False\n",
      "b'hastily'   False\n",
      "b'habitually'   False\n",
      "b'hastily'   False\n",
      "b'chronologically'   False\n",
      "b'temperate'   False\n",
      "b'cold'   True\n",
      "b'temperate'   False\n",
      "b'mild'   True\n",
      "b'temperate'   False\n",
      "b'short'   True\n",
      "b'temperate'   False\n",
      "b'windy'   False\n",
      "b'grin'   False\n",
      "b'exercise'   True\n",
      "b'grin'   False\n",
      "b'rest'   True\n",
      "b'grin'   False\n",
      "b'joke'   True\n",
      "b'grin'   False\n",
      "b'smile'   True\n",
      "b'verbally'   False\n",
      "b'orally'   False\n",
      "b'verbally'   False\n",
      "b'overtly'   False\n",
      "b'verbally'   False\n",
      "b'fittingly'   False\n",
      "b'verbally'   False\n",
      "b'verbosely'   False\n",
      "b'physician'   True\n",
      "b'chemist'   False\n",
      "b'physician'   True\n",
      "b'pharmacist'   False\n",
      "b'essentially'   True\n",
      "b'eagerly'   False\n",
      "b'essentially'   True\n",
      "b'ordinarily'   False\n",
      "b'keen'   False\n",
      "b'useful'   True\n",
      "b'keen'   False\n",
      "b'simple'   True\n",
      "b'keen'   False\n",
      "b'famous'   True\n",
      "b'keen'   False\n",
      "b'sharp'   True\n",
      "b'situated'   True\n",
      "b'rotating'   False\n",
      "b'situated'   True\n",
      "b'emptying'   False\n",
      "b'principal'   True\n",
      "b'exceptional'   False\n",
      "b'built'   True\n",
      "b'financed'   False\n",
      "b'unlikely'   True\n",
      "b'improbable'   False\n",
      "b'unlikely'   True\n",
      "b'disagreeable'   False\n",
      "b'unlikely'   True\n",
      "b'unpopular'   False\n",
      "b'halfheartedly'   False\n",
      "b'customarily'   False\n",
      "b'halfheartedly'   False\n",
      "b'bipartisanly'   False\n",
      "b'halfheartedly'   False\n",
      "b'apathetically'   False\n",
      "b'halfheartedly'   False\n",
      "b'unconventionally'   False\n",
      "b'annals'   False\n",
      "b'homes'   True\n",
      "b'annals'   False\n",
      "b'trails'   True\n",
      "b'annals'   False\n",
      "b'chronicles'   True\n",
      "b'annals'   False\n",
      "b'songs'   True\n",
      "b'wildly'   True\n",
      "b'distinctively'   False\n",
      "b'wildly'   True\n",
      "b'mysteriously'   False\n",
      "b'wildly'   True\n",
      "b'abruptly'   False\n",
      "b'wildly'   True\n",
      "b'furiously'   False\n",
      "b'hailed'   False\n",
      "b'judged'   True\n",
      "b'hailed'   False\n",
      "b'acclaimed'   False\n",
      "b'hailed'   False\n",
      "b'remembered'   True\n",
      "b'hailed'   False\n",
      "b'addressed'   True\n",
      "b'command'   True\n",
      "b'mastery'   False\n",
      "b'concocted'   False\n",
      "b'devised'   True\n",
      "b'concocted'   False\n",
      "b'cleaned'   False\n",
      "b'concocted'   False\n",
      "b'requested'   True\n",
      "b'concocted'   False\n",
      "b'supervised'   True\n",
      "b'prospective'   True\n",
      "b'prudent'   False\n",
      "b'generally'   True\n",
      "b'descriptively'   False\n",
      "b'generally'   True\n",
      "b'controversially'   False\n",
      "b'sustained'   True\n",
      "b'lowered'   False\n",
      "b'perilous'   False\n",
      "b'binding'   True\n",
      "b'perilous'   False\n",
      "b'exciting'   True\n",
      "b'perilous'   False\n",
      "b'offensive'   True\n",
      "b'perilous'   False\n",
      "b'dangerous'   True\n",
      "b'tranquillity'   False\n",
      "b'peacefulness'   False\n",
      "b'tranquillity'   False\n",
      "b'harshness'   False\n",
      "b'tranquillity'   False\n",
      "b'weariness'   False\n",
      "b'tranquillity'   False\n",
      "b'happiness'   True\n",
      "b'dissipate'   False\n",
      "b'disperse'   False\n",
      "b'dissipate'   False\n",
      "b'isolate'   True\n",
      "b'dissipate'   False\n",
      "b'disguise'   False\n",
      "b'dissipate'   False\n",
      "b'photograph'   True\n",
      "b'primarily'   True\n",
      "b'cautiously'   False\n",
      "b'primarily'   True\n",
      "b'chiefly'   False\n",
      "b'colloquial'   False\n",
      "b'recorded'   True\n",
      "b'colloquial'   False\n",
      "b'misunderstood'   False\n",
      "b'colloquial'   False\n",
      "b'incorrect'   True\n",
      "b'colloquial'   False\n",
      "b'conversational'   False\n",
      "b'resolved'   True\n",
      "b'publicized'   False\n",
      "b'feasible'   True\n",
      "b'equitable'   False\n",
      "b'expeditiously'   False\n",
      "b'frequently'   True\n",
      "b'expeditiously'   False\n",
      "b'actually'   True\n",
      "b'expeditiously'   False\n",
      "b'rapidly'   True\n",
      "b'expeditiously'   False\n",
      "b'repeatedly'   True\n",
      "b'terminated'   False\n",
      "b'ended'   True\n",
      "b'terminated'   False\n",
      "b'posed'   True\n",
      "b'terminated'   False\n",
      "b'postponed'   False\n",
      "b'terminated'   False\n",
      "b'evaluated'   True\n",
      "b'fashion'   True\n",
      "b'ration'   False\n",
      "b'fashion'   True\n",
      "b'fathom'   False\n",
      "b'fashion'   True\n",
      "b'craze'   False\n",
      "b'marketed'   False\n",
      "b'frozen'   True\n",
      "b'marketed'   False\n",
      "b'sold'   True\n",
      "b'marketed'   False\n",
      "b'sweetened'   False\n",
      "b'marketed'   False\n",
      "b'diluted'   True\n",
      "b'bigger'   True\n",
      "b'steadier'   False\n",
      "b'roots'   True\n",
      "b'rituals'   False\n",
      "b'normally'   True\n",
      "b'haltingly'   False\n",
      "b'normally'   True\n",
      "b'ordinarily'   False\n",
      "b'normally'   True\n",
      "b'permanently'   False\n",
      "Accuracy:  0.4375\n"
     ]
    }
   ],
   "source": [
    "with open('/home/TDDE09/labs/l5x/data/toefl.txt') as fp:\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    for line in fp:\n",
    "        distances = []\n",
    "        elements = line.split()\n",
    "        word = elements[0]\n",
    "        correct = int(elements[1])\n",
    "        for i,other_word in enumerate(elements[2:]):\n",
    "            distances.append(embedding.distance(str.encode(word),str.encode(other_word)))\n",
    "        pred = np.argmax(distances)\n",
    "        predictions.append(pred)\n",
    "        ground_truths.append(correct)\n",
    "predictions = np.array(predictions)\n",
    "ground_truths = np.array(ground_truths)\n",
    "accuracy = np.sum(predictions == ground_truths)/len(predictions)\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
