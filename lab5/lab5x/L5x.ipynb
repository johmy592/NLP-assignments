{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**Due date:** 2018-02-23\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Semantic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **word embedding** represents words as vectors in a high-dimensional vector space. In this lab you will train word embeddings on the English Wikipedia via truncated singular value decomposition of the co-occurrence matrix.\n",
    "\n",
    "Start by loading the Python module for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp5\n",
    "import scipy\n",
    "import numpy as np\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(source):\n",
    "    for sentence in source:\n",
    "        for token in sentence.split():\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the use of the generator function to obtain the tokens is essential here &ndash; returning the tokens as a list would require a lot of memory. If you have not worked with generators and iterators before, now is a good time to read up on them. [More information about generators](https://wiki.python.org/moin/Generators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(source, threshold=100):\n",
    "    # TODO: Replace the following line with your own code\n",
    "    V = {}\n",
    "    token_count = {}\n",
    "    for t in tokens(source):\n",
    "        token_count[t] = token_count.get(t, 0) + 1\n",
    "    i = 0\n",
    "    for t in token_count:\n",
    "        if t not in V and token_count[t] >= threshold:\n",
    "            V[t] = i\n",
    "            i+=1\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9313\n"
     ]
    }
   ],
   "source": [
    "with bz2.open('/home/TDDE09/labs/l5x/data/oanc.txt.bz2') as source:\n",
    "    vocab = make_vocab(source)\n",
    "    print(len(vocab))\n",
    "    #print(vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to help you test your implementation, we provide a smaller data file with the first 1M tokens from the full data. The code in the next cell builds the word-to-index mapping for this file and prints the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contexts(source, k):\n",
    "    # TODO: Replace the following line with your own code\n",
    "    \n",
    "    for sentence in source:\n",
    "        \n",
    "        s_list = sentence.split()\n",
    "        for i in range(len(s_list)):\n",
    "            context = []\n",
    "            for pos in range(i-k,i+k+1):\n",
    "                if pos < 0: \n",
    "                    context.append('<bos>')\n",
    "                elif pos >= len(s_list):\n",
    "                    context.append('<eos>')\n",
    "                else:\n",
    "                    context.append(s_list[pos])\n",
    "            yield tuple(context)\n",
    "    #yield from nlp5.contexts(source, k)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your code, you can run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ppmi_matrix(vocab, source, k=2, delta=1.0):\n",
    "    # TODO: Replace the following line with your own code\n",
    "    contexts_list = []\n",
    "    for context in contexts(list(source),k):\n",
    "        contexts_list.append(context)\n",
    "    context_matrix = np.zeros((len(vocab),len(vocab)))\n",
    "    N = 0\n",
    "    for ctxt in contexts_list:\n",
    "        word = ctxt[k]\n",
    "        if word not in vocab:\n",
    "            continue\n",
    "        context_words = [w for w in ctxt if w != word and w in vocab]\n",
    "        N += len(context_words)\n",
    "                         \n",
    "        for w in context_words:\n",
    "            context_matrix[vocab[word], vocab[w]] += 1\n",
    "\n",
    "    w_count = np.sum(context_matrix, axis=1, keepdims=True)\n",
    "    c_count = np.sum(context_matrix, axis=0, keepdims=True)\n",
    "    \n",
    "    norm_cm = context_matrix * N / w_count.dot(c_count)\n",
    "\n",
    "    # Set zero valued elements to 1 (log(1) == 0)\n",
    "    norm_cm[norm_cm == 0] = 1\n",
    "    \n",
    "    log_norm_cm = np.log(norm_cm) - np.log(delta)\n",
    "    \n",
    "    # Set negative values to zero\n",
    "    log_norm_cm[log_norm_cm < 0] = 0\n",
    "    \n",
    "    return scipy.sparse.csr_matrix(log_norm_cm)\n",
    "\n",
    "def ref_ppmi_matrix(vocab, source, k=2, delta=1.0):\n",
    "    return nlp5.make_ppmi_matrix(vocab, source, k, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open('/home/TDDE09/labs/l5x/data/oanc.txt.bz2') as source:\n",
    "    ppmi_matrix = make_ppmi_matrix(vocab, source)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You should now be able to obtain the PPMI as in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "scipy.sparse.save_npz('simplewnorm_context_matrixiki.npz', ppmi_matrix)\n",
    "#ppmi_matrix = scipy.sparse.load_npz('simplewnorm_context_matrixiki.npz')\n",
    "\n",
    "#with open('vocab.json') as json_file:\n",
    "#    vocab = json.loads(json_file.read())\n",
    "    \n",
    "#print (vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing the PPMI matrix will require approximately 60&nbsp;MB of disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the PPMI matrix, you can construct the word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 4</div>\n",
    "<div class=\"panel-body\">\n",
    "Implement a class representing word embeddings. The class should support the construction of the embedding by means of truncated singular value decomposition.\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "More specifically, we ask you to implement the following interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "class Embedding(object):\n",
    "    \"\"\"A word embedding.\n",
    "\n",
    "    A word embedding represents words as vectors in a d-dimensional\n",
    "    vector space. The central attribute of the word embedding is a\n",
    "    dense matrix whose rows correspond to the words in some\n",
    "    vocabulary, and whose columns correspond to the d dimensions in\n",
    "    the embedding space.\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "        vocab: The vocabulary, specified as a dictionary that maps\n",
    "            words to integer indices, identifying rows in the embedding\n",
    "            matrix.\n",
    "        dim: The dimensionality of the word vectors.\n",
    "        m: The embedding matrix. The rows of this matrix correspond to the\n",
    "            words in the vocabulary.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab, matrix, dim=100):\n",
    "        \"\"\"Initialies a new word embedding.\n",
    "\n",
    "        Args:\n",
    "            vocab: The vocabulary for the embedding, specified as a\n",
    "                dictionary that maps words to integer indices.\n",
    "            matrix: The co-occurrence matrix, represented as a SciPy\n",
    "                sparse matrix with one row and one column for each word in\n",
    "                the vocabulary.\n",
    "            dim: The dimensionality of the word vectors.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.dim = dim\n",
    "        self.svd = TruncatedSVD(n_components=dim)\n",
    "        self.m = self.svd.fit_transform(matrix)\n",
    "\n",
    "    def vec(self, w):\n",
    "        \"\"\"Returns the vector for the specified word.\n",
    "\n",
    "        Args:\n",
    "            w: A word, an element of the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            The word vector for the specified word.\n",
    "        \"\"\"\n",
    "        # TODO: Replace the following line with your own code\n",
    "        return self.m[self.vocab[w]]\n",
    "\n",
    "    def distance(self, w1, w2):\n",
    "        \"\"\"Computes the cosine similarity between the specified words.\n",
    "        \n",
    "        Args:\n",
    "            w1: The first word (an element of the vocabulary).\n",
    "            w2: The second word (an element of the vocabulary).\n",
    "        \n",
    "        Returns:\n",
    "            The cosine similarity between the specified words in this\n",
    "            embedding.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # So we can compare vectors with words in analogy()\n",
    "            if type(w1).__module__ == \"numpy\":\n",
    "                v1 = w1\n",
    "            else:\n",
    "                v1 = self.vec(w1)\n",
    "            v2 = self.vec(w2)\n",
    "\n",
    "            return v1.dot(v2.T) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        except:\n",
    "            #print(w1,\" \", str(w1 in self.vocab))\n",
    "            #print(w2,\" \", str(w2 in self.vocab))\n",
    "            return 0\n",
    "    \n",
    "    def most_similar(self, w, n=10):\n",
    "        \"\"\"Returns the most similar words for the specified word.\n",
    "\n",
    "        Args:\n",
    "            w: A word, an element of the vocabulary.\n",
    "            n: The maximal number of most similar words to return.\n",
    "\n",
    "        Returns:\n",
    "            A list containing distance/word pairs.\n",
    "        \"\"\"\n",
    "        words = list(self.vocab.keys())\n",
    "        distances = [self.distance(w, w_) for w_ in words]\n",
    "        similarity_indices = np.argsort(distances)[::-1]\n",
    "        similar_words = [words[i] for i in similarity_indices[:n]]\n",
    "        return similar_words\n",
    "\n",
    "    def analogy(self, w1, w2, w3):\n",
    "        \"\"\"Answers an analogy question of the form w1 - w2 + w3 = ?\n",
    "\n",
    "        Args:\n",
    "            w1: A word, an element of the vocabulary.\n",
    "            w2: A word, an element of the vocabulary.\n",
    "            w3: A word, an element of the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            The word closest to the vector w1 - w2 + w3 that is different\n",
    "            from all the other words.\n",
    "        \"\"\"\n",
    "        v1 = self.vec(w1)\n",
    "        v2 = self.vec(w2)\n",
    "        v3 = self.vec(w3)\n",
    "        \n",
    "        v = v1 - v2 + v3\n",
    "        \n",
    "        most_sim = self.most_similar(v, n=4)\n",
    "        for w in most_sim:\n",
    "            if w not in [w1,w2,w3]:\n",
    "                return w\n",
    "        \n",
    "        #norm_context_matrixnorm_context_matrixnorm_context_matrixreturn most_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the **singular value decomposition** of an $m \\times n$ matrix $\\mathbf{M}$ is a factorization of the form $\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^*$ where $\\mathbf{U}$ is an $m \\times m$ matrix of which we may assume that its columns are sorted in decreasing order of importance when it comes to explaining the variance of $\\mathbf{M}$. (Formally, these columns correspond to the singular values in the matrix $\\mathbf{\\Sigma}$.) By truncating $\\mathbf{U}$ after the first $\\mathit{dim}$ columns, we thus obtain an approximation of the original matrix $\\mathbf{M}$. In your case, $\\mathbf{M}$ is the PPMI matrix, and the truncated matrix $\\mathbf{U}$ gives the word vectors of the embedding. To compute the matrix $\\mathbf{U}$, you can use the class [TruncatedSVD](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html), which is available in [scikit-learn](http://scikit-learn.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows how to initalise a new embedding with the PPMI matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9313, 9313)\n"
     ]
    }
   ],
   "source": [
    "print (ppmi_matrix.shape)\n",
    "embedding = Embedding(vocab, ppmi_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some things that you can do with the embedding:\n",
    "\n",
    "#### Word similarity\n",
    "\n",
    "What is the semantic similarity between &lsquo;man&rsquo; and &lsquo;woman&rsquo;?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.918921016019255\n",
      "0.8158151704019476\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print (embedding.distance(b'man', b'woman'))\n",
    "print (embedding.distance(b'man', b'guy'))\n",
    "print (embedding.distance(b'man', b'banana'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What words are most similar to &lsquo;man&rsquo;?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'man', b'woman', b'boy', b'girl', b'person', b'father', b'guy', b'mother', b'wife', b'dead']\n"
     ]
    }
   ],
   "source": [
    "print (embedding.most_similar(b'man'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What words are most similar to &lsquo;woman&rsquo;?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'woman', b'man', b'girl', b'person', b'boy', b'mother', b'lady', b'father', b'daughter', b'men']\n"
     ]
    }
   ],
   "source": [
    "print (embedding.most_similar(b'woman'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analogies\n",
    "\n",
    "Here is the famous king &minus; man + woman = ? example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'prince'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.analogy(b'king', b'man', b'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When experimenting with other examples, you will find that the embedding picks up common stereotypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'nurse'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.analogy(b'doctor', b'man', b'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model knows the capital of Sweden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding.analogy(b'berlin', b'germany', b'sweden')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding also &lsquo;learns&rsquo; some syntactic analogies, such as the analogy between the past-tense and present-tense forms of verbs (here: *jump* and *eat*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding.analogy(b'jumped', b'jump', b'eat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.425\n"
     ]
    }
   ],
   "source": [
    "with open('/home/TDDE09/labs/l5x/data/toefl.txt') as fp:\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    for line in fp:\n",
    "        distances = []\n",
    "        elements = line.split()\n",
    "        word = elements[0]\n",
    "        correct = int(elements[1])\n",
    "        for i,other_word in enumerate(elements[2:]):\n",
    "            distances.append(embedding.distance(str.encode(word),str.encode(other_word)))\n",
    "        pred = np.argmax(distances)\n",
    "        predictions.append(pred)\n",
    "        ground_truths.append(correct)\n",
    "predictions = np.array(predictions)\n",
    "ground_truths = np.array(ground_truths)\n",
    "accuracy = np.sum(predictions == ground_truths)/len(predictions)\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
