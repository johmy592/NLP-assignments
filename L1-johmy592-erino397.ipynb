{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**Due date:** 2018-01-26\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1: Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will implement and compare the performance of two simple text classifiers: a Naive Bayes classifier and a classifier based on the averaged perceptron. Both of these classifiers are presented in the lecture.\n",
    "\n",
    "The data set that you will be using in this lab is the [review polarity data set](https://www.cs.cornell.edu/people/pabo/movie-review-data/) first used by [Pang and Lee (2004)](http://www.aclweb.org/anthology/P04-1035). It consists of 2,000 movie reviews, each of which has been tagged as either positive or negative towards the movie at hand. The distribution of the two classes is 50/50."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the module for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp1\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell loads the training data and the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = nlp1.load_data(\"/home/TDDE09/labs/l1/data/review_polarity.train.json\")\n",
    "test_data = nlp1.load_data(\"/home/TDDE09/labs/l1/data/review_polarity.test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will see, each data instance is a pair whose first component is a document, represented as a list of words (strings), and whose second component is the gold-standard polarity of the review (either positive `pos` or negative `neg`), represented as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['this', 'film', 'is', 'extraordinarily', 'horrendous', 'and', \"i'm\", 'not', 'going', 'to', 'waste', 'any', 'more', 'words', 'on', 'it', '.'], 'neg')\n"
     ]
    }
   ],
   "source": [
    "print(training_data[813])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that you will have to do is to implement a function\n",
    "\n",
    "`accuracy(classifier, data)`\n",
    "\n",
    "that computes the accuracy of a classifier on reference data of the form described above. In this context, a *classifier* is an object with a method `predict` that takes a document $x$ as its input and returns the predicted class for&nbsp;$x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(classifier, data):\n",
    "    \"\"\"Computes the accuracy of a classifier on reference data.\n",
    "\n",
    "    Args:\n",
    "        classifier: A classifier.\n",
    "        data: Reference data.\n",
    "\n",
    "    Returns:\n",
    "        The accuracy of the classifier on the test data, a float.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    for x,y in data:\n",
    "        pred = classifier.predict(x)\n",
    "        if pred == y:\n",
    "            correct+=1\n",
    "    #print(nlp1.accuracy(classifier,data))\n",
    "    return correct/len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test your function by computing the accuracy of a Naive Bayes classifier on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765\n"
     ]
    }
   ],
   "source": [
    "classifier1 = nlp1.NaiveBayes.train(training_data)\n",
    "print(accuracy(classifier1, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 1</div>\n",
    "<div class=\"panel-body\">\n",
    "Provide your own implementation of the `accuracy()` function in the code cell above. Test your implementation by redoing the evaluation. You should get exactly the same result as before.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the Naive Bayes classifier, you can start from the following code skeleton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(data):\n",
    "    V = set([word for art,c in data for word in art])\n",
    "    return V\n",
    "\n",
    "class NaiveBayes(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises a new classifier.\"\"\"\n",
    "        # TODO: Replace the following line with your own code\n",
    "        self.class_prob = {}\n",
    "        self.V = {}\n",
    "        self.word_prob = {}\n",
    "        self.class_dict = {}\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predicts the class for a document.\n",
    "\n",
    "        Args:\n",
    "            x: A document, represented as a list of words.\n",
    "\n",
    "        Returns:\n",
    "            The predicted class, represented asets a string.\n",
    "        \"\"\"\n",
    "        current_prediction = {'c':None,'prob':-float(\"inf\")}\n",
    "        for c in self.class_prob:\n",
    "            prob = math.log(self.class_prob[c])\n",
    "            for word in x:\n",
    "                if word in self.word_prob[c]:\n",
    "                    prob += math.log(self.word_prob[c][word])\n",
    "                # If word was not encountered during training for class c but is\n",
    "                # in vocabulary => set to default value\n",
    "                elif word in self.V:\n",
    "                    prob += math.log(self.default_word_prob[c])\n",
    "            if prob > current_prediction['prob']:\n",
    "                current_prediction = {'c':c,'prob':prob}\n",
    "        return current_prediction['c']\n",
    "\n",
    "    \n",
    "    def calculate_prior(self,data):\n",
    "        \n",
    "        for c in self.class_dict:\n",
    "            ccount = len(self.class_dict[c])\n",
    "            self.class_prob[c] = ccount/len(data)\n",
    "            \n",
    "    def get_class_dict(self,data):\n",
    "        unique_c = set([c for x,c in data])\n",
    "        for c in unique_c:\n",
    "            samples = [x for x,c_ in data if c_ == c]\n",
    "            self.class_dict[c] = samples\n",
    "            \n",
    "        \n",
    "    def calculate_word_prob(self,data,k):\n",
    "        self.default_word_prob = {}\n",
    "        for c in self.class_dict:\n",
    "            # Count of words for sentences with class c\n",
    "            word_count = defaultdict(int)\n",
    "            # Total count of words in class c\n",
    "            total_count = 0\n",
    "            for sample in self.class_dict[c]:\n",
    "                for word in set(sample):\n",
    "                    word_count[word] += sample.count(word)\n",
    "                total_count += len(sample)\n",
    "                \n",
    "            # For words in test set not encountered for class c during training\n",
    "            self.default_word_prob[c] = k / (total_count + len(self.V)*k)\n",
    "            \n",
    "            # Calculate probabilities of words given class c\n",
    "            self.word_prob[c] = {}\n",
    "            for word in word_count:\n",
    "                self.word_prob[c][word] = (word_count[word]+k)/(total_count + len(self.V)*k)\n",
    "    \n",
    "    def train(self, data, k=1):\n",
    "        \"\"\"Train a new classifier on training data using maximum\n",
    "        likelihood estimation and additive smoothing.\n",
    "\n",
    "        Args:\n",
    "            cls: The Python class representing the classifier.\n",
    "            data: Training data.\n",
    "            k: The smoothing constant.\n",
    "\n",
    "        Returns:\n",
    "            A trained classifier, an instance of `cls`.\n",
    "        \"\"\"\n",
    "        # TODO: Replace the following line with your own code\n",
    "        \n",
    "        self.V = get_vocabulary(data)\n",
    "        self.get_class_dict(data)\n",
    "        self.calculate_prior(data)\n",
    "        self.calculate_word_prob(data,k)\n",
    "        \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should meet the following requirements:\n",
    "\n",
    "### Number of classes\n",
    "\n",
    "Your implementation should support classification problems with an arbitrary number of classes. In particular, you should not hardwire the two classes used in the specific data set for this problem (`pos` and `neg`).\n",
    "\n",
    "### Vocabulary\n",
    "\n",
    "Your implementation should support the dynamic creation of the classifier&rsquo;s vocabulary from the training data. The vocabulary of the trained classifier should be the set of all words that occur in the training data.\n",
    "\n",
    "### Use log probabilities\n",
    "\n",
    "While the mathematical model of the Naive Bayes classifier is specified in terms of probabilities, for the implementation you should use log probabilities.\n",
    "\n",
    "### Test your implementation\n",
    "\n",
    "Test your implementation by evaluating on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-65e8e90a4e6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-1af30b965689>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(classifier, data)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mcorrect\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-23cd9f2404ad>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                     \u001b[0mprob\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m# If word was not encountered during training for class c but is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier2 = NaiveBayes()\n",
    "classifier2.train(training_data,1)\n",
    "\n",
    "\n",
    "print(accuracy(classifier2, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 2</div>\n",
    "<div class=\"panel-body\">\n",
    "Finish the implementation of the `NaiveBayes` class. Test your implementation by evaluating on the test data. When choosing the smoothing constant as$k=1$, you should get exactly the same results as in Problem&nbsp;1. What happens when you experiment with different values for the smoothing constant? Report your results and provide a short discussion in the text cell below.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* If we try with no smoothing, i.e. k=0 we get a ValueError due to our prediction attempting to calculate log(0).\n",
    "* If we try to gradualy decrease the value of k we get gradual improvements to accuracy up until and including k=0.5, which gives an accuracy of 0.78. If we then try to decrease k even further to k=4.5 we start to get worse predictions again. \n",
    "* If we try values larger than 1 we get gradualy worse accuracy. This means that the ideal smoothing for our classifier with this set of test data is around 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaged perceptron classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code skeleton for the averaged perceptron classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises a new classifier.\"\"\"\n",
    "        self.weights = {}\n",
    "        self.acc = {}\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predicts the class for a document.\n",
    "\n",
    "        Args:\n",
    "            x: A document, represented as a list of words.\n",
    "\n",
    "        Returns:\n",
    "            The predicted class, represented as a string.\n",
    "        \"\"\"\n",
    "        pred = {'c': None,'activation': -float(\"inf\")}\n",
    "        for c in self.weights:\n",
    "            out = sum([self.weights[c][word] for word in x if word in self.weights[c]])\n",
    "            if out > pred['activation']:\n",
    "                pred = {'c': c,'activation': out}\n",
    "            # Currently prefers classes that occur later in alphabet (same as reference model)\n",
    "            elif out == pred['activation'] and c > pred['c']:\n",
    "                pred = {'c': c,'activation': out}\n",
    "        return pred['c']    \n",
    "\n",
    "    def update(self, x, y):\n",
    "        \"\"\"Updates the weight vectors with a single training instance.\n",
    "\n",
    "        Args:\n",
    "            x: A document, represented as a list of words.\n",
    "            y: The gold-standard class, represented as a string.\n",
    "\n",
    "        Returns:\n",
    "            The predicted class, represented as a string.\n",
    "        \"\"\"\n",
    "        p = self.predict(x)       \n",
    "        if  p != y:\n",
    "            for word in x:\n",
    "                self.acc[p][word] -= self.count * 1\n",
    "                self.acc[y][word] += self.count * 1\n",
    "                self.weights[p][word] -= 1\n",
    "                self.weights[y][word] += 1\n",
    "        self.count += 1     \n",
    "        return p\n",
    "    \n",
    "    def init_weights(self, X, y):        \n",
    "        for c in set(y):\n",
    "            self.weights[c] = defaultdict(int)\n",
    "            self.acc[c] = defaultdict(int)\n",
    "            #for word in self.V:\n",
    "            #    self.weights[c][word] = 0\n",
    "            #    self.acc[c][word] = 0\n",
    "\n",
    "    def train(self, data, n_epochs=1):\n",
    "        \"\"\"Train a new classifier on training data using the averagedelif out == current_prediction['activation'] and c < current_prediction['c']:\n",
    "                current_prediction = {'c': c,'activation': out}\n",
    "        perceptron learning algorithm.\n",
    "\n",
    "        Args:gradual\n",
    "            cls: The Python class representing the classifier.\n",
    "            data: Training data.\n",
    "            n_epochs: The number of training epochs.\n",
    "\n",
    "        Returns:\n",
    "            A trained classifier, an instance of `cls`.\n",
    "        \"\"\"\n",
    "        self.V = list(get_vocabulary(data))\n",
    "        X = [X for X, _ in data]\n",
    "        y = [y for _, y in data]\n",
    "\n",
    "        self.count = 1\n",
    "        self.init_weights(X, y)\n",
    "        for epoch in range(n_epochs):\n",
    "            for x, c in zip(X, y): \n",
    "                self.update(x, c)\n",
    "        for c in self.weights:\n",
    "            for word in self.weights[c]:\n",
    "                self.weights[c][word] -= self.acc[c][word] / self.count\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your implementation should meet the following requirements:\n",
    "\n",
    "### Number of classes\n",
    "\n",
    "As in the case of the Naive Bayes classifier, your implementation of the multi-class perceptron should support classification problems with an arbitrary number of classes, not just the two classes from the review data set.\n",
    "\n",
    "### Features\n",
    "\n",
    "As the features for your classifier, you should use all words that occur in the training data (bag-of-words features). The weight of a feature should be the number of times the corresponding word occurs in the document.\n",
    "\n",
    "### Vector operations\n",
    "\n",
    "To implement the perceptron, you will have to translate between the mathematical model (which is formulated in terms of vectors) and the implementation in Python that was suggested in the lecture, where feature vectors are represented as lists and weight vectors are represented as dictionaries. In particular, you will have to think about how to implement the relevant vector operations on this representation.\n",
    "\n",
    "### Tie-breaking\n",
    "\n",
    "The exact results that you will get with your implementation will depend on how you break ties between classes with the same activation. For the sake of comparability, we ask you to adopt the following strategy: If more than one class get the same activation, pick the smallest class with respect to the lexicographic ordering on class names (so `neg` will come before `pos`).\n",
    "\n",
    "### Test your implementation\n",
    "\n",
    "To test your implementation, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier3 = nlp1.Perceptron.train(training_data)\n",
    "print(accuracy(classifier3, test_data))\n",
    "\n",
    "classifier4 = Perceptron()\n",
    "classifier4.train(training_data)\n",
    "\n",
    "print(accuracy(classifier4, test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 3</div>\n",
    "<div class=\"panel-body\">\n",
    "    <p>Finish the implementation of the averaged perceptron classifier. Test your implementation by evaluating on the test data. You should get exactly the same results as the reference implementation.</p>\n",
    "    <p>Run experiments to address the following questions:</p>\n",
    "    <ul>\n",
    "        <li>What happens when you repeat the experiment but do not do averaging?</li>\n",
    "        <li>What happens when you train the classifier for two epochs?</li>\n",
    "        <li>What happens when you invert the tie-breaking strategy?</li>\n",
    "    </ul>\n",
    "    <p>Report your results and provide a short discussion in the text cell below.</p>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Suppose that after the first 100 examples the weights vector is so good that no updates happen for the next 9899 examples and the perceptron predicts the last sample wrong, this will lead to the weights being updated with regard to that sample while not considering the previous correctly classified samples.\n",
    "    * When running our model without averaging we get an accuray of 0.64 instead of 0.745\n",
    "<br>\n",
    "<br>\n",
    "* The weights are further adapted during the second epoch and the running average counter is increased further. If we don't have access to a lot of data a risk is that the model is overfitted with regard to the training data and the performance would therefore be worse on the test data.\n",
    "    * If we run our model for 2 epochs we get an accuracy of 0.79\n",
    "<br>\n",
    "<br>\n",
    "* The classifier will make different predictions based off the tie-breaker strategy. If the developers want to set a default prediction in case of ties setting a tie-breaker strategy is one way of accomplishing this.\n",
    "    * We get an accuracy of 0.73 when inverting the tie-breaker strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching to binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lab so far, a document is represented as a list of the words that occur in it. For sentiment classification, several authors have suggested that a *binary* document representation, where each word is represented only once, can produce better results. In the last problem you will try to confirm this finding.\n",
    "\n",
    "Your task is to implement a function `binarize()` that converts data into the binary representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binarize(data):\n",
    "    # For each article extract the unique words\n",
    "    new_data = [(list(set(art)), c) for art, c in data]\n",
    "    return new_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is to be used in the following context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarized_training_data = binarize(training_data)\n",
    "binarized_test_data = binarize(test_data)\n",
    "\n",
    "classifier5 = NaiveBayes()\n",
    "classifier5.train(binarized_training_data)\n",
    "print(accuracy(classifier5, binarized_test_data))\n",
    "\n",
    "classifier6 = Perceptron()\n",
    "classifier6.train(binarized_training_data)\n",
    "print(accuracy(classifier6, binarized_test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 4</div>\n",
    "<div class=\"panel-body\">\n",
    "Implement the `binarize()` function and run the evaluation. What do you observe? Report your results and speculate on possible explanations in the text cell below.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By limit words in the data to one occurence per article we remove many occurences of a lot of words such as 'it', 'a' and 'is. These words are not helpful when classifying an article as positive or negative. By removing this factor we reduce noise in the data. We observed that by binarizing the data we got a higher accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
