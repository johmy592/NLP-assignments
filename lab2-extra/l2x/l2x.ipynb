{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp2\n",
    "import ngrams\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_charmap(filename):\n",
    "    vocab = set()\n",
    "    pro_dict = defaultdict(list)\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            chinese_char, pron = line.split()\n",
    "            pro_dict[pron].append(chinese_char)\n",
    "            vocab.add(chinese_char)\n",
    "    return pro_dict, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484\n"
     ]
    }
   ],
   "source": [
    "charmap, vocab = load_charmap('data/charmap')\n",
    "print (len(charmap['yi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese_tokenize(text):\n",
    "    tokens = []\n",
    "    token = ''\n",
    "    for c in text:\n",
    "        # Accumulate letters and numbers\n",
    "        if re.match(r'[a-zA-Z_0-9]', c):\n",
    "            token += c\n",
    "        # For other characters add to token list\n",
    "        else:\n",
    "            if token:\n",
    "                tokens.append(token)\n",
    "                token = ''\n",
    "\n",
    "            tokens.append(c.replace(' ', '<space>'))\n",
    "    # If we have accumulated letters or numbers add to tokens\n",
    "    if token:\n",
    "        tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Start and end tokens\n",
    "start = ['<s>']\n",
    "end = ['</s>']\n",
    "\n",
    "#for sample in training_data[-20:]:\n",
    "#    print (sample)\n",
    "\n",
    "training_data = []\n",
    "with open('data/train.han') as f:\n",
    "    for line in f:\n",
    "        # Tokenize\n",
    "        sample = chinese_tokenize(line.strip('\\n'))\n",
    "        # Add sample (with start and end tokens)\n",
    "        training_data.append(start + sample + end)\n",
    "        \n",
    "\n",
    "class WittenBellModel():\n",
    "    def __init__(self, order, training_data):\n",
    "        \n",
    "        # Get total number of tokens\n",
    "        self.N = sum([len(sent) for sent in training_data])\n",
    "\n",
    "        # Initialize models (up to order)\n",
    "        self.models = []\n",
    "        for i in range(1, order+1):\n",
    "            self.models.append(nlp2.train(ngrams.Model, i, training_data))\n",
    "            \n",
    "        self.n_grams = []\n",
    "        for i in range(1,order):\n",
    "            cur_grams = self.create_ngrams(i,training_data)\n",
    "            self.n_grams.append(cur_grams)\n",
    "        print(\"Done creating ngrams!\")\n",
    "            \n",
    "    def create_ngrams(self, n, data):\n",
    "        n_grams = {}\n",
    "        n_gram = ()\n",
    "        \n",
    "        for sentence in training_data:\n",
    "            for i in range(max(0,len(sentence)-n-1)):\n",
    "                n_plus_gram = tuple([sentence[j] for j in range(i,i+n+1)])\n",
    "                if n_plus_gram[:-1] not in n_grams:\n",
    "                    n_grams[n_plus_gram[:-1]] = [n_plus_gram]\n",
    "                else:\n",
    "                    n_grams[n_plus_gram[:-1]].append(n_plus_gram)\n",
    "                #print(n_gram)\n",
    "        for key in n_grams:\n",
    "            n_grams[key] = set(n_grams[key])\n",
    "        return n_grams\n",
    "            \n",
    "    def prob(self, ctxt, word):\n",
    "        model = self.models[len(ctxt)]\n",
    "        # If unigram (no context)\n",
    "        if not ctxt:\n",
    "            lambda_u = self.N / (self.N + len(model.vocabulary()))\n",
    "            # Try to calculate max likelihood\n",
    "            try:\n",
    "                max_likelihood = model.freq(ctxt, word)/self.N\n",
    "            except:\n",
    "                max_likelihood = 0\n",
    "            return lambda_u * max_likelihood + (1 - lambda_u) * (1 / len(model.vocabulary()))\n",
    "\n",
    "        # If bigram or higher\n",
    "        \n",
    "        \n",
    "        #----------------------------------------\n",
    "        \n",
    "        \n",
    "        #-----------------------------------------\n",
    "        \n",
    "        #-----------------------------------------\n",
    "        #v_model = self.models[len(ctxt)-1]\n",
    "        #for w in v_model.vocabulary():\n",
    "        #    if v_model.prob(ctxt[:-1],w):\n",
    "        #        v1 +=1\n",
    "        #print(\"V1: \", v1)\n",
    "        #------------------------------------------\n",
    "        \n",
    "        lambda_u = 0.5\n",
    "        #print(\"V1: \",v1)\n",
    "        #print(\"TOTAL: \", model.total(ctxt))\n",
    "        try:\n",
    "            v1 = len(self.n_grams[len(ctxt)-1][ctxt])\n",
    "            lambda_u = model.total(ctxt) / (model.total(ctxt) + v1)\n",
    "        except:\n",
    "            lambda_u = 0\n",
    "            \n",
    "        #print(lambda_u)\n",
    "        # Try to calculate max likelihood\n",
    "        try:\n",
    "            max_likelihood = model.freq(ctxt, word) / model.total(ctxt)\n",
    "        except:\n",
    "            max_likelihood = 0\n",
    "             \n",
    "        # Make recursive call\n",
    "        return lambda_u * max_likelihood + (1 - lambda_u) * self.prob(ctxt[1:], word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test pronounciations\n",
    "test_x = []\n",
    "with open('data/test.pin') as f:\n",
    "    for line in f:\n",
    "        test_x.append(line.strip('\\n').split()) \n",
    "\n",
    "# Read in chinese test characters\n",
    "test_y = []\n",
    "with open('data/test.han') as f:\n",
    "    for line in f:\n",
    "        test_y.append(line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done creating ngrams!\n",
      "0.8179366149696561\n"
     ]
    }
   ],
   "source": [
    "order = 4\n",
    "model = WittenBellModel(order, training_data)\n",
    "\n",
    "preds = []\n",
    "for sent in test_x:\n",
    "    pred = []\n",
    "    for i, word in enumerate(sent):\n",
    "        if len(word) == 1 or word not in charmap:\n",
    "            pred.append(word)\n",
    "            continue\n",
    "        \n",
    "        # Select best candidate\n",
    "        best = {'word': '', 'p': 0}\n",
    "        for cand in charmap[word]:\n",
    "            ctxt = ()\n",
    "            if order > 1:\n",
    "                pred_tokens = start + chinese_tokenize(pred)\n",
    "                if order-1 > len(pred_tokens):\n",
    "                    ctxt = tuple(pred_tokens)\n",
    "                else:\n",
    "                    ctxt = tuple(pred_tokens[-order+1:])\n",
    "                \n",
    "            p = model.prob(ctxt, cand)\n",
    "            if p > best['p']:\n",
    "                best = {'word': cand, 'p': p}\n",
    "        pred.append(best['word'])\n",
    "        \n",
    "    preds.append(\"\".join(pred).split('<space>'))\n",
    "\n",
    "# Calculate accuracy as frequency of correct classified characters\n",
    "correct = 0\n",
    "total_c = 0\n",
    "for pred, golden in zip(preds, test_y):\n",
    "    for p_w, g_w in zip(pred, golden):\n",
    "        for p_c, g_c in zip(p_w, g_w):\n",
    "            if p_c == g_c:\n",
    "                correct += 1\n",
    "            total_c += 1\n",
    "print (correct / total_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
