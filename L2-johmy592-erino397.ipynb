{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**Due date:** 2018-02-02\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2: Language modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will experiment with $n$-gram models. You will test various parameters that influence these models&rsquo; quality and train to estimate models with additive smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code import the Python modules needed for this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp2\n",
    "import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this lab consists of Arthur Conan Doyle&rsquo;s stories about Sherlock Holmes: *The Adventures of Sherlock Holmes*, *The Memoirs of Sherlock Holmes*, *The Return of Sherlock Holmes*, *His Last Bow* and *The Case-Book of Sherlock Holmes*. The next piece of code loads the first three of these as training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = nlp2.read_data(\"/home/TDDE09/labs/l2/data/advs.txt\",\n",
    "                               \"/home/TDDE09/labs/l2/data/mems.txt\",\n",
    "                               \"/home/TDDE09/labs/l2/data/retn.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is represented as a list of sentences, where one sentence is represented as a list of tokens (strings). The next line prints the 101th sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"', 'He', 'took', 'down', 'a', 'heavy', 'brown', 'volume', 'from', 'his', 'shelves', '.']\n"
     ]
    }
   ],
   "source": [
    "print(training_data[101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between a model’s quality and its order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of this lab you will examine the relation between an $n$-gram model’s quality and its **order**, i.e. the value of&nbsp;$n$. You will do both a qualitative and quantitative evaluation with the help of the entropy measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative evaluation\n",
    "\n",
    "The following line trains a bigram-model of the class `ngrams.Model` on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nlp2.train(ngrams.Model, 2, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model you are able to generate random sentences. Every time you run the following code cell a new sentence is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If I have my father's case of the lane now , to -- mental , and a little village policeman , heads .\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(model.generate()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the sentences. Do they sound natural?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 1</div>\n",
    "<div class=\"panel-body\">\n",
    "Train a unigram-, bigram-, trigram-, and quadrigram-model, and generate random sentences with each. How does the quality of the sentences change with the model’s order? Explain your observations using your understanding from how an $n$-gram model works. Use some generated sentences in order to illustrate your discussion. How would the sentences look like for higher values of $n$, such as $n=10$?\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the name wait no the operation can the : , Joseph\n",
      "My God help me for three lads in the windows would cover all at Coxon's manager .\n",
      "But here an instant the case , and almost to the end of the quiet thinker and logician of Baker Street .\n",
      "\" Instead of being ruined , my good sir , you will find a considerable quantity of straw , \" said he .\n",
      "\" \" Yes .\n"
     ]
    }
   ],
   "source": [
    "# TODO: Insert your code here\n",
    "models =[]\n",
    "for i in range(1,5):\n",
    "    models.append(nlp2.train(ngrams.Model, i, training_data))\n",
    "    print(\" \".join(models[-1].generate()))\n",
    "\n",
    "model2 = nlp2.train(ngrams.Model, 10, training_data)\n",
    "print(\" \".join(model2.generate()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We observe that the sentences get more natural as the value of n is increased. <br> \n",
    "For example: <br>\n",
    "n=1 gave us this: \"That's was he astonishing departs . preserved It that\" This is obviously not a very natural sentence. This is because a unigram model does not take any context into account, but simply looks at which words are common. <br>\n",
    "n=4 gave us: \"You have a few solid stepping-stones on which we may hope to get some data which may help us in our operations we erected a hydraulic press in excavating fuller's-earth , which , old as they were , too .\" This is a more natural sentence that begins to make sense. This is because in a quadrigram model the context of the 3 previous words is taken into account. <br><br>\n",
    "\n",
    "A bigger n, such as n=10 would improve this even further by taking more words into account. One can expect diminishing returns at some point when n gets quite large, for example there are no natural sentences with 1000 words.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative evaluation\n",
    "\n",
    "In order to do a quantitative evaluation of a model we can compute its **entropy** on held-out data. We will use the first part of the novel *The Adventures of Sherlock Holmes* for this. It is loaded by the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = nlp2.read_data(\"/home/TDDE09/labs/l2/data/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next piece of code trains a bigram-model and computes its entropy on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.426862596420277"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nlp2.train(ngrams.Model, 2, training_data)\n",
    "nlp2.evaluate(model, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 2</div>\n",
    "<div class=\"panel-body\">\n",
    "Compute the entropy for the four models you created for the previous problem. How and why does the model’s entropy change with the model’s order? Explain using your knowledge of the entropy measure.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 7.337551182974018\n",
      "2 3.426862596420277\n",
      "3 1.4289533769461726\n",
      "4 0.5436027106964166\n"
     ]
    }
   ],
   "source": [
    "# TODO: Insert your code here\n",
    "for i,model in enumerate(models):\n",
    "    print(i+1, nlp2.evaluate(model,test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase n the words will make more and more sense given the words that came before them. This is the result of the model considering more previous words when generating new ones, as n increases. Because of this there will be less surprises for larger n values, and therefore a smaller entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation between a model’s quality and the estimation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part of this lab you will implement and evaluate different estimation methods. When you called `nlp2.train()` you created an $n$-gram model, an instance of the class `ngrams.Model`, and trained this model using maximum likelihood estimation. To implement different estimation methods you will have to create instances of your own class of models.\n",
    "\n",
    "### The contents of a model\n",
    "\n",
    "The first step towards your own model class is to understand what methods are available inside a model. To this end, the next cell shows you skeleton code for your own `Model` class, which inherits from `ngrams.Model`. Note that you do not need to modify this code at this point; you should simply try to follow what&rsquo;s going on and try to understand how to use instances of the model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(ngrams.Model):\n",
    "    \n",
    "    def order(self):\n",
    "        \"\"\"Return the order of this model (an integer).\"\"\"\n",
    "        return super().order()\n",
    "    \n",
    "    def vocabulary(self):\n",
    "        \"\"\"Return this model's vocabulary (a set).\"\"\"\n",
    "        return super().vocabulary()\n",
    "    \n",
    "    def freq(self, ctxt, word):\n",
    "        \"\"\"Return the number of occurrences of `word` (a string) after `ctxt` (a tuple of strings).\"\"\"\n",
    "        return super().freq(ctxt, word)\n",
    "    \n",
    "    def total(self, ctxt):\n",
    "        \"\"\"Return the total number of ngrams that start with `ctxt` (a tuple of strings).\"\"\"\n",
    "        return super().total(ctxt)\n",
    "    \n",
    "    def prob(self, ctxt, word):\n",
    "        \"\"\"Return the probability for `word` (a string) given `ctxt` (a tuple of strings).\"\"\"\n",
    "        return super().prob(ctxt, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next cell trains a bigram-model of class `Model` and prints the model’s order (an integer) and the size of its vocabulary (a set of strings, represented by Python’s `set` type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order of the model: 2\n",
      "number of words in the model's vocabulary: 15339\n"
     ]
    }
   ],
   "source": [
    "model = nlp2.train(Model, 2, training_data)\n",
    "print(\"order of the model:\", model.order())\n",
    "print(\"number of words in the model's vocabulary:\", len(model.vocabulary()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up an n-gram’s absolute frequency\n",
    "\n",
    "A trained model consists primarily of a table with absolute frequencies for all $n$-grams that appear in the text it was trained on. In order to look up an $n$-gram’s absolute frequency you can use the method `freq()`. An $n$-gram is divided into two parts: an $(n-1)$-gram called **context** (`ctxt`) and a final unigram (`word`). In Python the context is represented as a tuple of strings and the unigram as a normal string.\n",
    "\n",
    "If you want to train a trigram model and then know the absolute frequency for the trigram *Mr. Sherlock Holmes* you can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nlp2.train(Model, 3, training_data)\n",
    "model.freq((\"Mr.\", \"Sherlock\"), \"Holmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training a bigram model and looking up the absolute frequency for the bigram *Baker Street* you can write the following. Note that the context of a bigram model is a 1-tuple of strings, which has a special notation in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nlp2.train(Model, 2, training_data)\n",
    "model.freq((\"Baker\",), \"Street\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look up the absolute frequency of an n-gram with a given context\n",
    "\n",
    "The method `total()` returns the absolute frequency of $n$-grams with the specified context. Here is an example for a trigram model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transverse\n"
     ]
    }
   ],
   "source": [
    "model = nlp2.train(Model, 3, training_data)\n",
    "model.total((\"Mr.\", \"Sherlock\"))\n",
    "print(model.vocabulary().pop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 3</div>\n",
    "<div class=\"panel-body\">\n",
    "Train a bigram model and use it to calculate the following values, using the methods shown above.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nlp2.train(Model, 2, training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1.** the absolute frequency for the bigram *Sherlock Holmes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.freq((\"Sherlock\",), \"Holmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.** the absolute frequency of bigrams with the context *Sherlock*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.total((\"Sherlock\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.** the absolute frequency for the unigram *Sherlock* &ndash; **note that you should still use the bigram model for this!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.total((\"Sherlock\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4.** the absolute frequency of trigrams with the context *Sherlock Holmes* &ndash; **note that you should still use the bigram model for this!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.freq((\"Sherlock\",), \"Holmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5.** the number of words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15339"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.6.** a list with all the unique words following the context *Sherlock*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n",
      "has\n",
      "everywhere\n",
      ".\n",
      "looked\n",
      "Holmes\n",
      "?\n",
      "Holmes's\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for word in model.vocabulary():\n",
    "    if model.freq((\"Sherlock\",),word):\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For 3.6 you will need to write a bit more than a simple function call.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate probabilities with the Maximum Likelihood method\n",
    "\n",
    "The method `prob()` returns the estimated conditional probability $P(w|c)$ for a word $w$ given a context $c$. The following code snippet trains a trigram model and estimates the pobability for *Holmes* given the context *Mr. Sherlock*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nlp2.train(Model, 3, training_data)\n",
    "model.prob((\"Mr.\", \"Sherlock\"), \"Holmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(What does the returned value imply?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 4</div>\n",
    "<div class=\"panel-body\">\n",
    "Do your own implementation of the method `prob()`. The method should estimate probabilities using the Maximum Likelihood method. You can call the methods that you used to solve Problem&nbsp;3. Test your implementation by redoing the evaluation from Problem&nbsp;2 with the new class `Model` instead of `ngrams.Model`. You should get the same results as before.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to solve this problem you will need to turn the formula for Maximum Likelihood estimation into code. We illustrate the formula for a bigram model. If we write $f(w_1w_2)$ for the number of occurrences of the bigram  $w_1w_2$ and $f(w_1)$ for the number of occurrences of the unigram $w_1$, then the probability for observing $w_2$ given $w_1$ is\n",
    "$$\n",
    "P(w_2|w_1) = \\frac{f(w_1w_2)}{f(w_1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(ngrams.Model):\n",
    "    \n",
    "    def prob(self, ctxt, word):\n",
    "        \"\"\"Return the probability for `word` (a string) given `ctxt` (a tuple of strings).\"\"\"\n",
    "        #print(\"Running new version of prob\\n\")\n",
    "        return super().freq(ctxt,word)/super().total(ctxt)\n",
    "\n",
    "\n",
    "model = nlp2.train(Model, 3, training_data)\n",
    "model.prob((\"Mr.\", \"Sherlock\"), \"Holmes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with Maximum Likelihood estimation\n",
    "\n",
    "The file `yoda.txt` contains the same text as `test.txt`, but in the jumbled [Yoda-language]( http://itre.cis.upenn.edu/~myl/languagelog/archives/002173.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoda_data = nlp2.read_data(\"/home/TDDE09/labs/l2/data/yoda.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 5</div>\n",
    "<div class=\"panel-body\">\n",
    "Redo the evaluation of the four previous models with `yoda.txt` as test data. For models with $n>1$ you get an error. Why? Explain what goes wrong based on your knowledge of Maximum Likelihood estimation.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 7.2441064060866385\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8afb1ac5055a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myoda_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/TDDE09/labs/l2/code/nlp2.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, test_data)\u001b[0m\n",
      "\u001b[0;32m/home/TDDE09/labs/l2/code/ngrams.pyc\u001b[0m in \u001b[0;36mlogprob\u001b[0;34m(self, ctxt, word)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "\n",
    "for i,model in enumerate(models):\n",
    "    print(i+1, nlp2.evaluate(model,yoda_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula used to calculate entropy uses log-probabilities as such: log(P(w1,w2,...,wn)) with P(w1,w2,...,wn) meaning the probablility of the sequence of words appearing together. This means that the evaluation of models with n>1 will crash if a never seen sequence of words appears since P(w1,....) will be 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate probabilities with additive smoothing\n",
    "\n",
    "For the next problem you are going to do Maximum Likelihood estimation, but with additive smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 6</div>\n",
    "<div class=\"panel-body\">\n",
    "<p>\n",
    "Write a new implementation of the method `prob()`, such that it estimates probabilities with additive smoothing.</p>\n",
    "<p>\n",
    "Evaluate the system on `test.txt` with new new class using the entropy measure from Problem&nbsp;2. Choose the following values for the smoothing constant $k$: 0.00, 0.01, 0.10, 1.00. For $k=0$ you should get the same results as in Problem&nbsp;4.\n",
    "</p>\n",
    "<p>\n",
    "Why and how does the smoothing constant influence the model’s entropy? Provide an explanation based on your understanding of what smoothing does to the distribution of the probability mass among observed and hallucinated occurrences.\n",
    "</p>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=1 k=0 Entropy: 7.337551182974018\n",
      "n=1 k=0.01 Entropy: 7.3366010522328216\n",
      "n=1 k=0.1 Entropy: 7.328460956945743\n",
      "n=1 k=1.0 Entropy: 7.273171406469247\n",
      "\n",
      "n=2 k=0 Entropy: 3.426862596420277\n",
      "n=2 k=0.01 Entropy: 4.813092274317304\n",
      "n=2 k=0.1 Entropy: 5.983399629244003\n",
      "n=2 k=1.0 Entropy: 7.383667454674711\n",
      "\n",
      "n=3 k=0 Entropy: 1.4289533769461726\n",
      "n=3 k=0.01 Entropy: 4.767917212034177\n",
      "n=3 k=0.1 Entropy: 6.733205552260075\n",
      "n=3 k=1.0 Entropy: 8.457347218116801\n",
      "\n",
      "n=4 k=0 Entropy: 0.5436027106964166\n",
      "n=4 k=0.01 Entropy: 4.854491062575155\n",
      "n=4 k=0.1 Entropy: 6.955629963762169\n",
      "n=4 k=1.0 Entropy: 8.657735266369459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Model(ngrams.Model):\n",
    "    \n",
    "    def prob(self, ctxt, word):\n",
    "        \"\"\"Return the probability for `word` (a string) given `ctxt` (a tuple of strings).\"\"\"\n",
    "        if not hasattr(self, 'k'):\n",
    "            self.k = 0\n",
    "       \n",
    "        return (super().freq(ctxt,word)+self.k)/(super().total(ctxt) + self.k*len(self.vocabulary()))\n",
    "\n",
    "\n",
    "for n in range(1, 5):\n",
    "    model = nlp2.train(Model, n, training_data)\n",
    "    for k in [0,0.01,0.10,1.00]:\n",
    "        model.k = k\n",
    "        print(\"n=%s k=%s Entropy: %s\" % (n, k, nlp2.evaluate(model,test_data)))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem, you can simply run your code multiple times, with different values of for&nbsp;$n$ and&nbsp;$k$. Enter your results into the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<tr><td></td><td>k = 0.00</td><td>k = 0.01</td><td>k = 0.10</td><td>k = 1.00</td></tr>\n",
    "<tr><td>n = 1</td><td>7.3375</td><td>7.3366</td><td>7.3284</td><td>7.2731</td></tr>\n",
    "<tr><td>n = 2</td><td>3.4268</td><td>4.8130</td><td>5.9833</td><td>7.3836</td></tr>\n",
    "<tr><td>n = 3</td><td>1.4289</td><td>4.7679</td><td>6.7332</td><td>8.4573</td></tr>\n",
    "<tr><td>n = 4</td><td>0.5436</td><td>4.8544</td><td>6.9556</td><td>8.6577</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "We notice that we get the same results as earlier when k = 0. Since k redistributes the probability distribution by essentially adding imaginary occurences of words given context, the entropy is increased for k values larger than zero. For n = 1 we notice that the entropy is slightly reduced as k increases, we reason that this is because the probability is evened out, and since the model does not take contextual information into account the model should not be confident in a particular word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An unseen test set\n",
    "\n",
    "Your last exercise is to redo the evaluation on a previously unseen test set, texts from the collection *His Last Bow*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_data = nlp2.read_data(\"/home/TDDE09/labs/l2/data/lstb.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"panel panel-primary\">\n",
    "<div class=\"panel-heading\">Problem 7</div>\n",
    "<div class=\"panel-body\">\n",
    "Redo the evaluation from Problem 6 with the new test data. Explain (without fixing anything) what happens given the differences between `test.txt` and `lstb.txt`.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'HIS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-a750a1f339fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.00\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"n=%s k=%s Entropy: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munseen_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/TDDE09/labs/l2/code/nlp2.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, test_data)\u001b[0m\n",
      "\u001b[0;32m/home/TDDE09/labs/l2/code/ngrams.pyc\u001b[0m in \u001b[0;36mlogprob\u001b[0;34m(self, ctxt, word)\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-cf29b3ecd643>\u001b[0m in \u001b[0;36mprob\u001b[0;34m(self, ctxt, word)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctxt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctxt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/TDDE09/labs/l2/code/ngrams.pyc\u001b[0m in \u001b[0;36mfreq\u001b[0;34m(self, ctxt, word)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HIS'"
     ]
    }
   ],
   "source": [
    "for n in range(1, 5):\n",
    "    model = nlp2.train(Model, n, training_data)\n",
    "    for k in [0,0.01,0.10,1.00]:\n",
    "        model.k = k\n",
    "        print(\"n=%s k=%s Entropy: %s\" % (n, k, nlp2.evaluate(model,unseen_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token 'HIS' does not occur in the training data and is therefore not in the vocabulary. This means that we get a KeyError when evaluating the model on unseen_data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
